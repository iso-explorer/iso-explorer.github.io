post_cb({"bq_ids": {"n4140": {"so_24708480_24708610_0": {"section_id": 988, "quality": 1.0, "length": 6}, "so_24708480_24708554_1": {"section_id": 989, "quality": 0.9166666666666666, "length": 22}}, "n3337": {"so_24708480_24708610_0": {"section_id": 973, "quality": 1.0, "length": 6}, "so_24708480_24708554_1": {"section_id": 974, "quality": 0.9166666666666666, "length": 22}}, "n4659": {"so_24708480_24708610_0": {"section_id": 1051, "quality": 1.0, "length": 6}, "so_24708480_24708554_1": {"section_id": 1052, "quality": 0.875, "length": 21}}}, "24708706": {"ParentId": "24708480", "PostTypeId": "2", "CommentCount": "0", "CreationDate": "2014-07-12T01:22:40.253", "Score": "0", "LastEditorUserId": "3466415", "LastEditDate": "2014-07-12T02:22:30.647", "Id": "24708706", "OwnerUserId": "3466415", "Body": "<p>The algorithm for allocating additional space as the vector grows has \"constant amortized complexity\" due to the notion that the total complexity (which is O(N) when a vector of N elements is created by a series of push_back() operations) can be \"amortized\" over the N push_back() calls--that is, the total cost is divided by N.</p>\n<p>Even more specifically, using the algorithm that allocates twice as much space each time, the worst case is that the algorithm allocates nearly 4 times as much memory as would need to be allocated if you knew the exact size of the vector in advance. The last allocation is just slightly less than two times the size of the vector after the allocation, and the some of all the previous allocations is slightly less than the size of the last allocation.\nThe total number of allocations is O(log N), and the number of deallocations (up to that point) is just one less than the number of allocations.</p>\n<p>For a large vector, if you know its maximum size in advance, it's more efficient to reserve that space at the beginning (one allocation rather than O(log N) allocations)\nbefore inserting any data.</p>\n<p>If you cut the capacity in half each time the size of the vector shrank to 1/4 of the currently-allocated space--that is, if you ran the allocation algorithm in reverse--you would be re-allocating (and then deallocating) nearly as much memory as the maximum capacity of the vector, <em>in</em> <em>addition</em> to deallocating the memory block with the maximum capacity.  That's a performance penalty for applications that simply wanted to erase elements of the vector until they were all gone and then delete the vector.</p>\n<p>That is, with deallocation as well as allocation, it's better to do it all at once if you can. And with deallocation you (almost) always can.</p>\n<p>The only beneficiary of the more complicated deallocation algorithm would be an application that makes a vector, then erases at least 3/4 of it and then <em>keeps</em> <em>the</em> <em>remaining</em> <em>part</em> <em>in</em> <em>memory</em> while proceeding to grow new vectors.  And even then there would be no benefit from the complicated algorithm unless the sum of the maximum capacities of the old (but still existing) vectors and the new vectors was so large that the application started to run into limitations of virtual memory.</p>\n<p>Why penalize all algorithms that progressively erase their vectors in order to gain this advantage in this special case?</p>\n", "LastActivityDate": "2014-07-12T02:22:30.647"}, "24708610": {"ParentId": "24708480", "CommentCount": "0", "CreationDate": "2014-07-12T01:03:04.857", "OwnerUserId": "103167", "PostTypeId": "2", "Id": "24708610", "Score": "2", "Body": "<p>Even more than the performance of moving all elements is the effect on existing iterators and pointers to elements.  The behavior of <code>erase</code> is:</p>\n<blockquote>\n<p id=\"so_24708480_24708610_0\">Invalidates iterators and references at or after the point of the erase.</p>\n</blockquote>\n<p>If reallocation occurred, then all iterators, pointers, and references would become invalid.  In general, keeping iterator validity is a desirable thing.</p>\n", "LastActivityDate": "2014-07-12T01:03:04.857"}, "24708480": {"CommentCount": "3", "ViewCount": "274", "CreationDate": "2014-07-12T00:36:36.263", "LastActivityDate": "2014-07-12T02:22:30.647", "Title": "why std::vector item deletion does not reduce its capacity?", "PostTypeId": "1", "Id": "24708480", "Score": "1", "Body": "<p>I am aware that when we insert items to a vector its capacity could be increase by non-linear factor. In gcc its capacity doubles. But I wonder why when I erase items from a vector, the capacity does not reduce. I tried to find out a reason for this. It 'seems' C++ standard does not say any word about this reduction (either to do or not). </p>\n<p>For my understand ideally, when vector size comes to 1/4 of its capacity at item deletion, it the vector could be shrunken by 1/2 of its capacity to achieve constant amortized space allocation/de-allocation complexity.</p>\n<p>My question is why C++ standard does not specify capacity reduction policy? What are the language design goals to not to specify anything about this? Does anyone has an idea about this? </p>\n", "Tags": "<c++><vector>", "OwnerUserId": "3219193", "AnswerCount": "3"}, "24708554": {"ParentId": "24708480", "PostTypeId": "2", "CommentCount": "2", "CreationDate": "2014-07-12T00:50:06.067", "Score": "4", "LastEditorUserId": "241631", "LastEditDate": "2014-07-12T00:55:11.013", "Id": "24708554", "OwnerUserId": "241631", "Body": "<blockquote>\n<p id=\"so_24708480_24708554_0\">It 'seems' C++ standard does not say any word about this reduction (either to do or not)</p>\n</blockquote>\n<p>This is not true, because the complexity description for <code>vector::erase</code> specifies exactly what operations will be performed.</p>\n<p><em>From \u00a723.3.6.5/4 [vector.modifiers]</em></p>\n<blockquote>\n<pre><code> iterator erase(const_iterator position);\n iterator erase(const_iterator first, const_iterator last);\n</code></pre>\n<p id=\"so_24708480_24708554_1\"><em>Complexity:</em> The destructor of <code>T</code> is called the number of times equal to the number of the elements erased, but the move assignment operator of <code>T</code> is called the number of times equal to the number of elements in the vector after the erased elements.</p>\n</blockquote>\n<p>This precludes implementations from reducing capacity because that would mean reallocation of storage and moving all remaining elements to the new memory.</p>\n<hr>\n<p>And if you're asking why the standard itself doesn't specify implementations are allowed to reduce capacity when you erase elements, then one can only guess the reasons. </p>\n<ul>\n<li><p>It was probably considered not important enough from a performance point of view to have the <code>vector</code> spend time reallocating and moving elements when erasing</p></li>\n<li><p>Reducing capacity would also add an additional possibility of an exception due to a failed memory allocation.</p></li>\n</ul>\n<hr>\n<p>You can attempt to reduce capacity yourself by calling <code>vector::shrink_to_fit</code>, but be aware that this call is non-binding, and implementations are allowed to ignore it.</p>\n<p>Another possibility for reducing the capacity would be move the elements into a temporary <code>vector</code> and <code>swap</code> it back into the original.</p>\n<pre><code>decltype(vec)(std::make_move_iterator(vec.begin()), \n              std::make_move_iterator(vec.end())).swap(vec);\n</code></pre>\n<p>But even with the second method, there's nothing stopping an implementation from over allocating storage.</p>\n</hr></hr>", "LastActivityDate": "2014-07-12T00:55:11.013"}});