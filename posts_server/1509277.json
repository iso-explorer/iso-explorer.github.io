post_cb({"bq_ids": {"n4140": {"so_1509277_1509474_3": {"length": 14, "quality": 0.875, "section_id": 6659}, "so_1509277_1509474_4": {"length": 14, "quality": 0.875, "section_id": 6660}, "so_1509277_1509474_1": {"length": 26, "quality": 0.8387096774193549, "section_id": 6507}, "so_1509277_1509474_2": {"length": 14, "quality": 0.875, "section_id": 6658}}, "n3337": {"so_1509277_1509474_3": {"length": 14, "quality": 0.875, "section_id": 6414}, "so_1509277_1509474_4": {"length": 14, "quality": 0.875, "section_id": 6415}, "so_1509277_1509474_1": {"length": 28, "quality": 0.9032258064516129, "section_id": 6262}, "so_1509277_1509474_2": {"length": 14, "quality": 0.875, "section_id": 6413}}, "n4659": {"so_1509277_1509474_3": {"length": 14, "quality": 0.875, "section_id": 6437}, "so_1509277_1509474_1": {"length": 25, "quality": 0.8064516129032258, "section_id": 7989}, "so_1509277_1509474_4": {"length": 14, "quality": 0.875, "section_id": 6438}, "so_1509277_1509474_2": {"length": 14, "quality": 0.875, "section_id": 6436}}}, "1509474": {"Id": "1509474", "PostTypeId": "2", "Body": "<p>A very partial answer for the first question: A file <em>is</em> a sequence of bytes so, when dealing with <code>wchar_t</code>'s, at least <em>some</em> conversion between <code>wchar_t</code> and <code>char</code> must occur. Making this conversion \"intelligently\" requires knowledge of the character encodings, so this is why this conversion is allowed to be locale-dependent, by virtue of using a facet in the stream's locale.</p>\n<p>Then, the question is how that conversion should be made in the only locale required by the standard: the \"classic\" one. There is no \"right\" answer for that, and the standard is thus very vague about it. I understand from your question that you assume that blindly casting (or memcpy()-ing) between wchar_t[] and char[] would have been a good way. This is not unreasonable, and is in fact what is (or at least was) done in some implementations.</p>\n<p>Another POV would be that, since a codecvt is a locale facet, it is reasonable to expect that the conversion is made using the \"locale's encoding\" (I'm handwavy here, as the concept is pretty fuzzy). For example, one would expect a Turkish locale to use ISO-8859-9, or a Japanese on to use Shift JIS. By similarity, the \"classic\" locale would convert to this \"locale's encoding\". Apparently, Microsoft chose to simply trim (which leads to IS-8859-1 if we assuming that <code>wchar_t</code> represents UTF-16 and that we stay in the basic multilingual plane), while the Linux implementation I know about decided stick to ASCII.</p>\n<p>For your second question:</p>\n<blockquote>\n<p id=\"so_1509277_1509474_0\">Also, are we gonna get real unicode streams with C++0x or am I missing something here?</p>\n</blockquote>\n<p>In the [locale.codecvt] section of n2857 (the latest C++0x draft I have at hand), one can read:</p>\n<blockquote>\n<p id=\"so_1509277_1509474_1\">The specialization <code>codecvt&lt;char16_t, char, mbstate_t&gt;</code> converts between the UTF-16 and UTF-8 encodings schemes, and the specialization <code>codecvt &lt;char32_t, char, mbstate_t&gt;</code> converts between the UTF-32 and UTF-8 encodings schemes. <code>codecvt&lt;wchar_t,char,mbstate_t&gt;</code> converts between the native character sets for narrow and wide characters.</p>\n</blockquote>\n<p>In the [locale.stdcvt] section, we find:</p>\n<blockquote>\n<p id=\"so_1509277_1509474_2\">For the facet <code>codecvt_utf8</code>:\n  \u2014 The facet shall convert between UTF-8 multibyte sequences and UCS2 or UCS4 (depending on the size of Elem) within the program.\n  [...]</p>\n<p id=\"so_1509277_1509474_3\">For the facet <code>codecvt_utf16</code>:\n  \u2014 The facet shall convert between UTF-16 multibyte sequences and UCS2 or UCS4 (depending on the size of Elem) within the program.\n  [...]</p>\n<p id=\"so_1509277_1509474_4\">For the facet <code>codecvt_utf8_utf16</code>:\n  \u2014 The facet shall convert between UTF-8 multibyte sequences and UTF-16 (one or two 16-bit codes) within the program.</p>\n</blockquote>\n<p>So I guess that this means \"yes\", but you'd have to be more precise about what you mean by \"real unicode streams\" to be sure.</p>\n", "LastEditorUserId": "59781", "LastActivityDate": "2009-10-02T17:27:36.000", "Score": "13", "CreationDate": "2009-10-02T13:21:05.703", "ParentId": "1509277", "CommentCount": "2", "OwnerUserId": "59781", "LastEditDate": "2009-10-02T17:27:36.000"}, "3468554": {"Id": "3468554", "PostTypeId": "2", "Body": "<p>Check this out:\n<a href=\"http://msdn.microsoft.com/en-us/library/tzf8k3z8(v=VS.80).aspx\" rel=\"nofollow noreferrer\">Class basic_filebuf</a></p>\n<p>You can alter the default behavior by setting a <em>wide</em> char buffer, using pubsetbuf.\nOnce you did that, the output will be wchar_t and not char.</p>\n<p>In other words for your example you will have:</p>\n<pre><code>wofstream file(L\"Test.txt\", ios_base::binary); //binary is important to set!  \nwchar_t buffer[128];  \nfile.rdbuf()-&gt;pubsetbuf(buffer, 128);  \nfile.put(0xFEFF); //this is the BOM flag, UTF16 needs this, but mirosoft's UNICODE doesn't, so you can skip this line, if any.  \nfile &lt;&lt; someString; // the output file will consist of unicode characters! without the call to pubsetbuf, the out file will be ANSI (current regional settings)  \n</code></pre>\n", "LastEditorUserId": "1285431", "LastActivityDate": "2014-07-29T16:36:58.587", "Score": "3", "CreationDate": "2010-08-12T14:08:25.517", "ParentId": "1509277", "CommentCount": "0", "OwnerUserId": "418498", "LastEditDate": "2014-07-29T16:36:58.587"}, "1510143": {"Id": "1510143", "PostTypeId": "2", "Body": "<p>The model used by C++ for charsets is inherited from C, and so dates back to at least 1989.</p>\n<p>Two main points:</p>\n<ul>\n<li>IO is done in term of char.</li>\n<li>it is the job of the locale to determine how wide chars are serialized</li>\n<li>the default locale (named \"C\") is very minimal (I don't remember the constraints from the standard, here it is able to handle only 7-bit ASCII as narrow and wide character set).</li>\n<li>there is an environment determined locale named \"\"</li>\n</ul>\n<p>So to get anything, you have to set the locale.</p>\n<p>If I use the simple program</p>\n<pre><code>#include &lt;locale&gt;\n#include &lt;fstream&gt;\n#include &lt;ostream&gt;\n#include &lt;iostream&gt;\n\nint main()\n{\n    wchar_t c = 0x00FF;\n    std::locale::global(std::locale(\"\"));\n    std::wofstream os(\"test.dat\");\n    os &lt;&lt; c &lt;&lt; std::endl;\n    if (!os) {\n        std::cout &lt;&lt; \"Output failed\\n\";\n    }\n}\n</code></pre>\n<p>which use the environment locale and output the wide character of code 0x00FF to a file.  If I ask to use the \"C\" locale, I get</p>\n<pre><code>$ env LC_ALL=C ./a.out\nOutput failed\n</code></pre>\n<p>the locale has been unable to handle the wide character and we get notified of the problem as the IO failed.  If I run ask an UTF-8 locale, I get</p>\n<pre><code>$ env LC_ALL=en_US.utf8 ./a.out\n$ od -t x1 test.dat\n0000000 c3 bf 0a\n0000003\n</code></pre>\n<p>(od -t x1 just dump the file represented in hex), exactly what I expect for an UTF-8 encoded file.</p>\n", "LastActivityDate": "2009-10-02T15:10:10.237", "CommentCount": "6", "CreationDate": "2009-10-02T15:10:10.237", "ParentId": "1509277", "Score": "7", "OwnerUserId": "136208"}, "1509483": {"Id": "1509483", "PostTypeId": "2", "Body": "<p>I don't know about wofstream. But C++0x will include new distict character types (char16_t, char32_t) of guaranteed width and signedness (unsigned) which can be portably used for UTF-8, UTF-16 and UTF-32. In addition, there will be new string literals (u\"Hello!\" for an UTF-16 coded string literal, for example)</p>\n<p>Check out the most recent <a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2009/n2960.pdf\" rel=\"nofollow noreferrer\">C++0x draft (N2960)</a>.</p>\n", "LastEditorUserId": "172531", "LastActivityDate": "2009-10-02T13:34:16.673", "Score": "3", "CreationDate": "2009-10-02T13:22:42.330", "ParentId": "1509277", "CommentCount": "0", "OwnerUserId": "172531", "LastEditDate": "2009-10-02T13:34:16.673"}, "1509277": {"ViewCount": "7224", "Body": "<p>Honestly, I just don't get the following design decision in C++ Standard library. When writing wide characters to a file, the <code>wofstream</code> converts <code>wchar_t</code> into <code>char</code> characters:</p>\n<pre><code>#include &lt;fstream&gt;\n#include &lt;string&gt;\n\nint main()\n{\n    using namespace std;\n\n    wstring someString = L\"Hello StackOverflow!\";\n    wofstream file(L\"Test.txt\");\n\n    file &lt;&lt; someString; // the output file will consist of ASCII characters!\n}\n</code></pre>\n<p>I am aware that this has to do with the standard <code>codecvt</code>. There is <code>codecvt</code> for <code>utf8</code> in <a href=\"http://www.boost.org/doc/libs/1_40_0/libs/serialization/doc/codecvt.html\" rel=\"nofollow noreferrer\"><code>Boost</code></a>. Also, there is a <code>codecvt</code> for <code>utf16</code> by <a href=\"https://stackoverflow.com/questions/207662/writing-utf16-to-file-in-binary-mode/208431#208431\">Martin York here on SO</a>. The question is <em>why</em> the <code>standard codecvt</code> converts wide-characters? why not write the characters as they are!</p>\n<p>Also, are we gonna get real <code>unicode streams</code> with C++0x or am I missing something here?</p>\n", "AcceptedAnswerId": "1510143", "Title": "Why does wide file-stream in C++ narrow written data by default?", "CreationDate": "2009-10-02T12:39:56.393", "Id": "1509277", "CommentCount": "2", "FavoriteCount": "7", "PostTypeId": "1", "LastEditDate": "2017-05-23T12:07:04.890", "LastEditorUserId": "-1", "LastActivityDate": "2014-07-29T16:36:58.587", "Score": "18", "OwnerUserId": "127893", "Tags": "<c++><file><unicode><wofstream>", "AnswerCount": "5"}, "1510162": {"Id": "1510162", "PostTypeId": "2", "Body": "<p>For your first question, this is my guess. </p>\n<p>The IOStreams library was constructed under a couple of premises regarding encodings. For converting between Unicode and other not so usual encodings, for example, it's assumed that.</p>\n<ul>\n<li>Inside your program, you should use a (fixed-width) wide-character encoding.</li>\n<li>Only external storage should use (variable-width) multibyte encodings.</li>\n</ul>\n<p>I believe that is the reason for the existence of the two template specializations of std::codecvt. One that maps between char types (maybe you're simply working with ASCII) and another that maps between wchar_t (internal to your program) and char (external devices). So whenever you need to perform a conversion to a multibyte encoding you should do it byte-by-byte. Notice that you can write a facet that handles encoding state when you read/write each byte from/to the multibyte encoding.</p>\n<p>Thinking this way the behavior of the C++ standard is understandable. After all, you're using wide-character ASCII encoded (assuming this is the default on your platform and you did not switch locales) strings. The \"natural\" conversion would be to convert each wide-character ASCII character to a ordinary (in this case, one char) ASCII character. (The conversion exists and is straightforward.)</p>\n<p>By the way, I'm not sure if you know, but you can avoid this by creating a facet that returns <em>noconv</em> for the conversions. Then, you would have your file with wide-characters.</p>\n", "LastEditorUserId": "155191", "LastActivityDate": "2009-10-02T15:21:24.463", "Score": "2", "CreationDate": "2009-10-02T15:13:41.417", "ParentId": "1509277", "CommentCount": "2", "OwnerUserId": "155191", "LastEditDate": "2009-10-02T15:21:24.463"}});