post_cb({"bq_ids": {"n4140": {"so_18223161_18430772_2": {"length": 7, "quality": 0.7777777777777778, "section_id": 2711}, "so_18223161_18430772_1": {"length": 18, "quality": 0.9, "section_id": 5831}, "so_18223161_18430772_0": {"length": 35, "quality": 1.0, "section_id": 1227}}, "n3337": {"so_18223161_18430772_2": {"length": 7, "quality": 0.7777777777777778, "section_id": 2672}, "so_18223161_18430772_1": {"length": 18, "quality": 0.9, "section_id": 5602}, "so_18223161_18430772_0": {"length": 35, "quality": 1.0, "section_id": 1225}}, "n4659": {"so_18223161_18430772_2": {"length": 7, "quality": 0.7777777777777778, "section_id": 3451}, "so_18223161_18430772_1": {"length": 18, "quality": 0.9, "section_id": 7293}, "so_18223161_18430772_0": {"length": 35, "quality": 1.0, "section_id": 1316}}}, "18566096": {"Id": "18566096", "PostTypeId": "2", "Body": "<p>This example gets at a variation of reads-from-thin-air like behavior.  The relevant discussion in the spec is in section 29.3p9-11.  Since the current version of the C11 standard doesn't guarantee dependences be respected, the memory model should allow the assertion to be fired.  The most likely situation is that the compiler optimizes away the check that a_local&gt;=0.  But even if you replace that check with a signal fence, CPUs would be free to reorder those instructions.\nYou can test such code examples under the C/C++11 memory models using the open source CDSChecker tool.\nThe interesting issue with your example is that for an execution to violate the assertion, there has to be a cycle of dependences.  More concretely:</p>\n<p>The b.fetch_add in thread one depends on the a.fetch_add in the same loop iteration due to the if condition.  The a.fetch_add  in thread 2 depends on b.load.  For an assertion violation, we have to have T2's b.load read from a b.fetch_add in a later loop iteration than T2's a.fetch_add.  Now consider the b.fetch_add the b.load reads from and call it # for future reference. We know that b.load depends on # as it takes it value from #.</p>\n<p>We know that # must depend on T2's a.fetch_add as T2's a.fetch_add atomic reads and updates a prior a.fetch_add from T1 in the same loop iteration as #.  So we know that # depends on the a.fetch_add in thread 2.    That gives us a cycle in dependences and is plain weird, but allowed by the C/C++ memory model.  The most likely way of actually producing that cycle is (1) compiler figures out that a.local is always greater than 0, breaking the dependence.   It can then do loop unrolling and reorder T1's fetch_add however it wants.</p>\n", "LastEditorUserId": "2364204", "LastActivityDate": "2013-09-03T05:33:52.063", "Score": "2", "CreationDate": "2013-09-02T05:18:02.527", "ParentId": "18223161", "CommentCount": "2", "OwnerUserId": "2364204", "LastEditDate": "2013-09-03T05:33:52.063"}, "18223161": {"ViewCount": "1798", "Body": "<p>I'm writing some lock-free code, and I came up with an interesting pattern, but I'm not sure if it will behave as expected under relaxed memory ordering.</p>\n<p>The simplest way to explain it is using an example:</p>\n<pre><code>std::atomic&lt;int&gt; a, b, c;\n\nauto a_local = a.load(std::memory_order_relaxed);\nauto b_local = b.load(std::memory_order_relaxed);\nif (a_local &lt; b_local) {\n    auto c_local = c.fetch_add(1, std::memory_order_relaxed);\n}\n</code></pre>\n<p>Note that all operations use <code>std::memory_order_relaxed</code>.</p>\n<p>Obviously, on the thread that this is executed on, the loads for <code>a</code> and <code>b</code> must be done before the <code>if</code> condition is evaluated.</p>\n<p>Similarly, the read-modify-write (RMW) operation on <code>c</code> must be done after the condition is evaluated (because it's conditional on that... condition).</p>\n<p>What I want to know is, does this code guarantee that the value of <code>c_local</code> is at least as up-to-date as the values of <code>a_local</code> and <code>b_local</code>? If so, how is this possible given the relaxed memory ordering? Is the control dependency together with the RWM operation acting as some sort of acquire fence? (Note that there's not even a corresponding release anywhere.)</p>\n<p>If the above holds true, I believe this example should also work (assuming no overflow) -- am I right?</p>\n<pre><code>std::atomic&lt;int&gt; a(0), b(0);\n\n// Thread 1\nwhile (true) {\n    auto a_local = a.fetch_add(1, std::memory_order_relaxed);\n    if (a_local &gt;= 0) {    // Always true at runtime\n        b.fetch_add(1, std::memory_order_relaxed);\n    }\n}\n\n// Thread 2\nauto b_local = b.load(std::memory_order_relaxed);\nif (b_local &lt; 777) {\n    // Note that fetch_add returns the pre-incrementation value\n    auto a_local = a.fetch_add(1, std::memory_order_relaxed);\n    assert(b_local &lt;= a_local);    // Is this guaranteed?\n}\n</code></pre>\n<p>On thread 1, there is a control dependency which I suspect guarantees that <code>a</code> is always incremented before <code>b</code> is incremented (but they each keep being incremented neck-and-neck). On thread 2, there is another control dependency which I suspect guarantees that <code>b</code> is loaded into <code>b_local</code> before <code>a</code> is incremented. I also think that the value returned from <code>fetch_add</code> will be at least as recent as any observed value in <code>b_local</code>, and the <code>assert</code> should therefore hold. But I'm not sure, since this departs significantly from the usual memory-ordering examples, and my understanding of the C++11 memory model is not perfect (I have trouble reasoning about these memory ordering effects with any degree of certainty). Any insights would be appreciated!</p>\n<hr>\n<p><strong>Update</strong>: As bames53 has helpfully pointed out in the comments, given a sufficiently smart compiler, it's possible that an <code>if</code> could be optimised out entirely under the right circumstances, in which case the relaxed loads could be reordered to occur after the RMW, causing their values to be more up-to-date than the <code>fetch_add</code> return value (the <code>assert</code> could fire in my second example). However, what if instead of an <code>if</code>, an <code>atomic_signal_fence</code> (not <code>atomic_thread_fence</code>) is inserted? That certainly can't be ignored by the compiler no matter what optimizations are done, but does it ensure that the code behaves as expected? Is the CPU allowed to do any re-ordering in such a case?</p>\n<p>The second example then becomes:</p>\n<pre><code>std::atomic&lt;int&gt; a(0), b(0);\n\n// Thread 1\nwhile (true) {\n    auto a_local = a.fetch_add(1, std::memory_order_relaxed);\n    std::atomic_signal_fence(std::memory_order_acq_rel);\n    b.fetch_add(1, std::memory_order_relaxed);\n}\n\n// Thread 2\nauto b_local = b.load(std::memory_order_relaxed);\nstd::atomic_signal_fence(std::memory_order_acq_rel);\n// Note that fetch_add returns the pre-incrementation value\nauto a_local = a.fetch_add(1, std::memory_order_relaxed);\nassert(b_local &lt;= a_local);    // Is this guaranteed?\n</code></pre>\n<hr>\n<p><strong>Another update</strong>: After reading all the responses so far and combing through the standard myself, I don't think it can be shown that the code is correct using only the standard. So, can anyone come up with a counter-example of a theoretical system that complies with the standard and also fires the assert?</p>\n</hr></hr>", "AcceptedAnswerId": "18566096", "Title": "What are the C++11 memory ordering guarantees in this corner case?", "CreationDate": "2013-08-14T04:22:17.043", "Id": "18223161", "CommentCount": "12", "FavoriteCount": "6", "PostTypeId": "1", "LastEditDate": "2013-08-31T06:23:21.730", "LastEditorUserId": "21475", "LastActivityDate": "2013-09-03T05:33:52.063", "Score": "19", "OwnerUserId": "21475", "Tags": "<multithreading><c++11><atomic><memory-model>", "AnswerCount": "2"}, "18430772": {"Id": "18430772", "PostTypeId": "2", "Body": "<p>Signal fences don't provide the necessary guarantees (well, not unless 'thread 2' is a signal hander that actually runs on 'thread 1').</p>\n<p>To guarantee correct behavior we need synchronization between threads, and the fence that does that is <code>std::atomic_thread_fence</code>.</p>\n<hr>\n<p>Let's label the statements so we can diagram various executions (with thread fences replacing signal fences, as required):</p>\n<pre><code>while (true) {\n    auto a_local = a.fetch_add(1, std::memory_order_relaxed); // A\n    std::atomic_thread_fence(std::memory_order_acq_rel);      // B\n    b.fetch_add(1, std::memory_order_relaxed);                // C\n}\n</code></pre>\n<p><br/></p>\n<pre><code>auto b_local = b.load(std::memory_order_relaxed);             // X\nstd::atomic_thread_fence(std::memory_order_acq_rel);          // Y\nauto a_local = a.fetch_add(1, std::memory_order_relaxed);     // Z\n</code></pre>\n<p><br/></p>\n<p>So first let's assume that <em>X</em> loads a value written by <em>C</em>. The following paragraph specifies that in that case the fences synchronize and a <em>happens-before</em> relationship is established.</p>\n<p><em>29.8/2:</em></p>\n<blockquote>\n<p id=\"so_18223161_18430772_0\">A release fence <em>A</em> synchronizes with an acquire fence <em>B</em> if there exist atomic operations <em>X</em> and <em>Y</em>, both operating on some atomic object <em>M</em>, such that <em>A</em> is sequenced before <em>X</em>, <em>X</em> modifies <em>M</em>, <em>Y</em> is sequenced before <em>B</em>, and <em>Y</em> reads the value written by <em>X</em> or a value written by any side effect in the hypothetical release sequence <em>X</em> would head if it were a release operation.</p>\n</blockquote>\n<p>And here's a possible execution order where the arrows are <em>happens-before</em> relations. </p>\n<pre><code>Thread 1: A\u2081 \u2192 B\u2081 \u2192 C\u2081 \u2192 A\u2082 \u2192 B\u2082 \u2192 C\u2082 \u2192 ...\n                \u2198\nThread 2:    X \u2192 Y \u2192 Z\n</code></pre>\n<blockquote>\n<p id=\"so_18223161_18430772_1\">If a side effect <em>X</em> on an atomic object <em>M</em> happens before a value computation <em>B</em> of <em>M</em>, then the evaluation <em>B</em> shall take its value from <em>X</em> or from a side effect <em>Y</em> that follows <em>X</em> in the modification order of <em>M</em>. <em>\u2014 [C++11 1.10/18]</em></p>\n</blockquote>\n<p>So the load at <em>Z</em> must take its value from <em>A\u2081</em> or from a subsequent modification. Therefore the assert holds because the value written at <em>A\u2081</em> and at all later modifications is greater than or equal to the value written at <em>C\u2081</em> (and read by <em>X</em>).</p>\n<hr>\n<p>Now let's look at the case where the fences do not synchronize. This happens when the load of <code>b</code> does not load a value written by thread 1, but instead reads the value that <code>b</code> is initialized with. There's still synchronization where the threads starts though:</p>\n<p>30.3.1.2/5</p>\n<blockquote>\n<p id=\"so_18223161_18430772_2\"><em>Synchronization</em>: The completion of the invocation of the constructor synchronizes with the beginning of the invocation of the copy of f.</p>\n</blockquote>\n<p>This is specifying the behavior of <code>std::thread</code>'s constructor. So (assuming the thread creation is correctly sequenced after the initialization of <code>a</code>) the value read by <em>Z</em> must take its value from the initialization of <code>a</code> or from one of the subsequent modifications on thread 1, which means that the assertions still holds.</p>\n</hr></hr>", "LastEditorUserId": "365496", "LastActivityDate": "2013-08-26T00:08:55.993", "Score": "3", "CreationDate": "2013-08-25T15:45:24.950", "ParentId": "18223161", "CommentCount": "5", "OwnerUserId": "365496", "LastEditDate": "2013-08-26T00:08:55.993"}});