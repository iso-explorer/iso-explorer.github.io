post_cb({"4234663": {"ParentId": "4234120", "PostTypeId": "2", "CommentCount": "4", "Body": "<p>There are many cases on this.</p>\n<ol>\n<li><p>Many hi-speed MPUs have barrel shifter, multiplexer-like electronic circuit which do any shift in constant time.</p></li>\n<li><p>If MPU have only 1 bit shift <code>x &lt;&lt; 10</code> would normally be slower, as it mostly done by 10 shifts or byte copying with 2 shifts.</p></li>\n<li><p>But there is known common case where <code>x &lt;&lt; 10</code> would be even <em>faster</em> than <code>x &lt;&lt; 1</code>. If x is 16 bit, only lower 6 bits of it is care (all other will be shifted out), so MPU need to load only lower byte, thus make only single access cycle to 8-bit memory, while <code>x &lt;&lt; 10</code> need two access cycles. If access cycle is slower than shift (and clear lower byte), <code>x &lt;&lt; 10</code> will be faster. This may apply to microcontrollers with fast onboard program ROM while accessing slow external data RAM.</p></li>\n<li><p>As addition to case 3, compiler may care about number of significant bits in <code>x &lt;&lt; 10</code> and optimize further operations to lower-width ones, like replacing 16x16 multiplication with 16x8 one (as lower byte is always zero).</p></li>\n</ol>\n<p>Note, some microcontrollers have no shift-left instruction at all, they use <code>add x,x</code> instead.</p>\n", "OwnerUserId": "484743", "LastEditorUserId": "484743", "LastEditDate": "2010-11-20T19:56:16.747", "Id": "4234663", "Score": "28", "CreationDate": "2010-11-20T19:51:14.440", "LastActivityDate": "2010-11-20T19:56:16.747"}, "4234312": {"ParentId": "4234120", "CommentCount": "3", "Body": "<p>On ARM, this can be done as a side effect of another instruction. So potentially, there's no latency at all for either of them.</p>\n", "OwnerUserId": "492716", "PostTypeId": "2", "Id": "4234312", "Score": "9", "CreationDate": "2010-11-20T18:33:05.510", "LastActivityDate": "2010-11-20T18:33:05.510"}, "46626217": {"ParentId": "4234120", "PostTypeId": "2", "CommentCount": "0", "Body": "<p><strong>As always, it depends on the surrounding code context</strong>: e.g. are you using <code>x&lt;&lt;1</code> as an array index?  Or adding it to something else?  In either case, small shift counts (1 or 2) can often optimize even more than if the compiler ends up having to <em>just</em> shift.  Not to mention the whole throughput vs. latency vs. front-end bottlenecks tradeoff.  Performance of a tiny fragment is not one-dimensional.</p>\n<p>A hardware shift instructions is not a compiler's only option for compiling <code>x&lt;&lt;1</code>, but the other answers are mostly assuming that.</p>\n<hr>\n<p><strong><code>x &lt;&lt; 1</code> is exactly equivalent to <code>x+x</code></strong> for unsigned, and for 2's complement signed integers.  Compilers always know what hardware they're targeting while they're compiling, so they can take advantage of tricks like this.</p>\n<p>On <a href=\"https://www.realworldtech.com/haswell-cpu/4/\" rel=\"nofollow noreferrer\">Intel Haswell</a>, <code>add</code> has 4 per clock throughput, but <code>shl</code> with an immediate count has only 2 per clock throughput.  (See <a href=\"http://agner.org/optimize/\" rel=\"nofollow noreferrer\">http://agner.org/optimize/</a> for instruction tables, and other links in the <a class=\"post-tag\" href=\"/questions/tagged/x86\" rel=\"tag\" title=\"show questions tagged 'x86'\">x86</a> tag wiki).  SIMD vector shifts are 1 per clock (2 in Skylake), but SIMD vector integer adds are 2 per clock (3 in Skylake).  Latency is the same, though: 1 cycle.</p>\n<p>There's also a special shift-by-one encoding of <code>shl</code> where the count is implicit in the opcode.  8086 didn't have immediate-count shifts, only by-one and by <code>cl</code> register.   This is mostly relevant for right-shifts, because you can just add for left shifts unless you're shifting a memory operand.  But if the value is needed later, it's better to load into a register first.  But anyway, <code>shl eax,1</code> or <code>add eax,eax</code> is one byte shorter than <code>shl eax,10</code>, and code-size can directly (decode / front-end bottlenecks) or indirectly (L1I code cache misses) affect performance.</p>\n<p>More generally, small shift counts can sometimes be optimized into a scaled index in an addressing mode on x86.  Most other architectures in common use these days are RISC, and don't have scaled-index addressing modes, but x86 is a common enough architecture for this to be worth mentioning.  (e.g.g if you're indexing an array of 4-byte elements, there's room to increase the scale factor by 1 for <code>int arr[]; arr[x&lt;&lt;1]</code>).</p>\n<hr>\n<p>Needing to copy+shift is common in situations where the original value of <code>x</code> is still needed.  But <strong>most x86 integer instructions operate in-place.</strong>  (The destination is one of the sources for instructions like <code>add</code> or <code>shl</code>.)  The x86-64 System V calling convention passes args in registers, with the first arg in <code>edi</code> and return value in <code>eax</code>, so a function that returns <code>x&lt;&lt;10</code> also makes the compiler emit copy+shift code.</p>\n<p>The <a href=\"https://stackoverflow.com/questions/46597055/address-computation-instruction-leaq/46597375#46597375\"><code>LEA</code> instruction lets you shift-and-add</a> (with a shift count of 0 to 3, because it uses addressing-mode machine-encoding).  It puts the result in a separate register.</p>\n<p><a href=\"https://gcc.godbolt.org/#g:!((g:!((g:!((h:codeEditor,i:(j:1,source:'int+shl1(int+x)+%7B+return+x%3C%3C1%3B+%7D%0A%0Aint+shl2(int+x)+%7B+return+x%3C%3C2%3B+%7D%0Aint+times5(int+x)+%7B+return+x+*+5%3B+%7D%0A%0Aint+shl10(int+x)+%7B+return+x%3C%3C10%3B+%7D%0A'),l:'5',n:'0',o:'C%2B%2B+source+%231',t:'0')),k:45.266594124047884,l:'4',n:'0',o:'',s:0,t:'0'),(g:!((h:compiler,i:(compiler:clang500,filters:(b:'0',binary:'1',commentOnly:'0',demangle:'0',directives:'0',execute:'1',intel:'0',trim:'1'),libs:!(),options:'-xc+-Wall+-O3',source:1),l:'5',n:'0',o:'x86-64+clang+5.0.0+(Editor+%231,+Compiler+%232)',t:'0')),header:(),k:54.73340587595213,l:'4',n:'0',o:'',s:0,t:'0')),l:'2',n:'0',o:'',t:'0')),version:4\" rel=\"nofollow noreferrer\">gcc and clang both optimize these functions the same way, as you can see on the Godbolt compiler explorer</a>:</p>\n<pre><code>int shl1(int x) { return x&lt;&lt;1; }\n    lea     eax, [rdi+rdi]   # 1 cycle latency, 1 uop\n    ret\n\nint shl2(int x) { return x&lt;&lt;2; }\n    lea     eax, [4*rdi]    # longer encoding: needs a disp32 of 0 because there's no base register, only scaled-index.\n    ret\n\nint times5(int x) { return x * 5; }\n    lea     eax, [rdi + 4*rdi]\n    ret\n\nint shl10(int x) { return x&lt;&lt;10; }\n    mov     eax, edi         # 1 uop, 0 or 1 cycle latency\n    shl     eax, 10          # 1 uop, 1 cycle latency\n    ret\n</code></pre>\n<p>LEA with 2 components has 1 cycle latency and 2-per-clock throughput on recent Intel and AMD CPUs.  (Sandybridge-family and Bulldozer/Ryzen).  On Intel, it's only 1 per clock throughput with 3c latency for <code>lea eax, [rdi + rsi + 123]</code>.  (Related: <a href=\"https://stackoverflow.com/questions/40354978/why-is-this-c-code-faster-than-my-hand-written-assembly-for-testing-the-collat/40355466#40355466\">Why is this C++ code faster than my hand-written assembly for testing the Collatz conjecture?</a> goes into this in detail.)</p>\n<p>Anyway, copy+shift by 10 needs a separate <code>mov</code> instruction.  It might be zero latency on many recent CPUs, but it still takes front-end bandwidth and code size.  (<a href=\"https://stackoverflow.com/questions/44169342/can-x86s-mov-really-be-free-why-cant-i-reproduce-this-at-all\">Can x86's MOV really be \"free\"? Why can't I reproduce this at all?</a>)</p>\n<p>Also related: <a href=\"https://stackoverflow.com/questions/46480579/how-to-multiply-a-register-by-37-using-only-2-consecutive-leal-instructions-in-x\">How to multiply a register by 37 using only 2 consecutive leal instructions in x86?</a>.</p>\n<hr>\n<p><strong>The compiler is also free to transform the surrounding code so there isn't an actual shift, or it's combined with other operations</strong>.</p>\n<p>For example <code>if(x&lt;&lt;1) { }</code> could use an <code>and</code> to check all bits except the high bit.  On x86, you'd use a <code>test</code> instruction, like <code>test eax, 0x7fffffff</code> / <code>jz .false</code> instead of <code>shl eax,1 / jz</code>.  This optimization works for any shift count, and it also works on machines where large-count shifts are slow (like Pentium 4), or non-existent (some micro-controllers).</p>\n<p>Many ISAs have bit-manipulation instructions beyond just shifting.  e.g. PowerPC has a lot of bit-field extract / insert instructions.  Or ARM has shifts of source operands as part of any other instruction.  (So shift/rotate instructions are just a special form of <code>move</code>, using a shifted source.)</p>\n<p>Remember, <strong>C is not assembly language</strong>.  Always look at <em>optimized</em> compiler output when you're tuning your source code to compile efficiently.</p>\n</hr></hr></hr>", "OwnerUserId": "224132", "LastEditorUserId": "224132", "LastEditDate": "2017-10-08T21:59:51.007", "Id": "46626217", "Score": "3", "CreationDate": "2017-10-07T23:50:20.723", "LastActivityDate": "2017-10-08T21:59:51.007"}, "4248013": {"ParentId": "4234120", "CommentCount": "0", "Body": "<p>Here's <a href=\"http://web.cecs.pdx.edu/~harry/Relay/\">my favorite CPU</a>, in which <code>x&lt;&lt;2</code> takes twice as long as <code>x&lt;&lt;1</code> :)</p>\n", "OwnerUserId": "23771", "PostTypeId": "2", "Id": "4248013", "Score": "9", "CreationDate": "2010-11-22T17:02:22.647", "LastActivityDate": "2010-11-22T17:02:22.647"}, "4234137": {"ParentId": "4234120", "CommentCount": "8", "Body": "<p>Potentially depends on the CPU.</p>\n<p>However, all modern CPUs (x86, ARM) use a \"barrel shifter\" -- a hardware module specifically designed to perform arbitrary shifts in constant time.</p>\n<p>So the bottom line is... no. No difference.</p>\n", "OwnerUserId": "23388", "PostTypeId": "2", "Id": "4234137", "Score": "82", "CreationDate": "2010-11-20T17:58:01.703", "LastActivityDate": "2010-11-20T17:58:01.703"}, "4235537": {"ParentId": "4234120", "PostTypeId": "2", "CommentCount": "5", "Body": "<p>That depends both on the CPU and compiler. Even if the underlying CPU has arbitrary bit shift with a barrel shifter, this will only happen if the compiler takes advantage of that resource.</p>\n<p>Keep in mind that shifting anything outside the width in bits of the data is \"undefined behavior\" in C and C++. Right shift of signed data is also \"implementation defined\". Rather than too much concern about speed, be concerned that you are getting the same answer on different implementations. </p>\n<p>Quoting from ANSI C section 3.3.7:</p>\n<blockquote>\n<p id=\"so_4234120_4235537_0\">3.3.7 Bitwise shift operators</p>\n<p id=\"so_4234120_4235537_1\">Syntax</p>\n<pre><code>      shift-expression:\n              additive-expression\n              shift-expression &lt;&lt;  additive-expression\n              shift-expression &gt;&gt;  additive-expression\n</code></pre>\n<p id=\"so_4234120_4235537_2\">Constraints</p>\n<p id=\"so_4234120_4235537_3\">Each of the operands shall have\n  integral type.  </p>\n<p id=\"so_4234120_4235537_4\">Semantics</p>\n<p id=\"so_4234120_4235537_5\">The integral promotions are\n  performed on each of the operands. \n  The type of the result is that of the\n  promoted left operand.  If the value\n  of the right operand is negative or is\n  greater than or equal to the width in\n  bits of the promoted left operand, the\n  behavior is undefined.</p>\n<p id=\"so_4234120_4235537_6\">The result of E1 &lt;&lt; E2 is E1\n  left-shifted E2 bit positions; vacated\n  bits are filled with zeros.  If E1 has\n  an unsigned type, the value of the\n  result is E1 multiplied by the\n  quantity, 2 raised to the power E2,\n  reduced modulo ULONG_MAX+1 if E1 has\n  type unsigned long, UINT_MAX+1\n  otherwise.  (The constants ULONG_MAX\n  and UINT_MAX are defined in the header\n   .)</p>\n<p id=\"so_4234120_4235537_7\">The result of E1 &gt;&gt; E2 is E1\n  right-shifted E2 bit positions.  If E1\n  has an unsigned type or if E1 has a\n  signed type and a nonnegative value,\n  the value of the result is the\n  integral part of the quotient of E1\n  divided by the quantity, 2 raised to\n  the power E2 .  If E1 has a signed\n  type and a negative value, the\n  resulting value is\n  implementation-defined.</p>\n</blockquote>\n<p>So:</p>\n<pre><code>x = y &lt;&lt; z;\n</code></pre>\n<p>\"&lt;&lt;\": y \u00d7 2<sup>z</sup> (<strong>undefined</strong> if an overflow occurs);</p>\n<pre><code>x = y &gt;&gt; z;\n</code></pre>\n<p>\"&gt;&gt;\": <strong>implementation-defined for signed</strong> (most often the result of the arithmetic shift: y / 2<sup>z</sup>).</p>\n", "OwnerUserId": "455276", "LastEditorUserId": "455276", "LastEditDate": "2010-11-22T00:28:48.663", "Id": "4235537", "Score": "7", "CreationDate": "2010-11-20T23:15:03.177", "LastActivityDate": "2010-11-22T00:28:48.663"}, "4234120": {"CommentCount": "6", "AcceptedAnswerId": "4234137", "PostTypeId": "1", "LastEditorUserId": "469935", "CreationDate": "2010-11-20T17:54:03.903", "LastActivityDate": "2017-10-08T21:59:51.007", "LastEditDate": "2011-08-06T10:21:14.300", "ViewCount": "6803", "FavoriteCount": "13", "Title": "Which is faster:  x<<1  or  x<<10?", "Id": "4234120", "Score": "81", "Body": "<p>I don't want to optimize anything, I swear, I just want to ask this question out of curiosity.\nI know that on most hardware there's an assembly command of bit-shift (e.g. <code>shl</code>, <code>shr</code>), which is a single command. But does it matter (nanosecond-wise, or CPU-tact-wise) how many bits you shift. In other words, is either of the following faster on any CPU?</p>\n<pre><code>x &lt;&lt; 1;\n</code></pre>\n<p>and</p>\n<pre><code>x &lt;&lt; 10;\n</code></pre>\n<p>And please don't hate me for this question. :)</p>\n", "Tags": "<c++><c><performance><cpu><low-level>", "OwnerUserId": "469935", "AnswerCount": "9"}, "4235096": {"ParentId": "4234120", "CommentCount": "2", "Body": "<p>On some generations of Intel CPUs (P2 or P3? Not AMD though, if I remember right), the bitshift operations are ridiculously slow. Bitshift by 1 bit should always be fast though since it can just use addition. Another question to consider is whether bitshifts by a constant number of bits are faster than variable-length shifts. Even if the opcodes are the same speed, on x86 the nonconstant righthand operand of a bitshift must occupy the CL register, which imposes additional constrains on register allocation and may slow the program down that way too.</p>\n", "OwnerUserId": "379897", "PostTypeId": "2", "Id": "4235096", "Score": "5", "CreationDate": "2010-11-20T21:28:41.840", "LastActivityDate": "2010-11-20T21:28:41.840"}, "4376068": {"ParentId": "4234120", "PostTypeId": "2", "CommentCount": "0", "Body": "<p>It is conceivable that, on an 8-bit processor, <code>x&lt;&lt;1</code> could actually be <em>much slower</em> than <code>x&lt;&lt;10</code> for a 16-bit value.</p>\n<p>For example a reasonable translation of <code>x&lt;&lt;1</code> may be:</p>\n<pre><code>byte1 = (byte1 &lt;&lt; 1) | (byte2 &gt;&gt; 7)\nbyte2 = (byte2 &lt;&lt; 1)\n</code></pre>\n<p>whereas <code>x&lt;&lt;10</code> would be more simple:</p>\n<pre><code>byte1 = (byte2 &lt;&lt; 2)\nbyte2 = 0\n</code></pre>\n<p>Notice how <code>x&lt;&lt;1</code> shifts more often and even farther than <code>x&lt;&lt;10</code>. Furthermore the result of <code>x&lt;&lt;10</code> doesn't depend on the content of byte1. This could speed up the operation additionally.</p>\n", "OwnerUserId": "526040", "LastEditorUserId": "526040", "LastEditDate": "2017-03-17T20:57:15.287", "Id": "4376068", "Score": "7", "CreationDate": "2010-12-07T11:23:18.590", "LastActivityDate": "2017-03-17T20:57:15.287"}, "bq_ids": {"n4140": {"so_4234120_4235537_3": {"section_id": 6138, "quality": 0.8, "length": 4}, "so_4234120_4235537_7": {"section_id": 6148, "quality": 0.7407407407407407, "length": 20}, "so_4234120_4235537_5": {"section_id": 6146, "quality": 0.6923076923076923, "length": 18}}, "n3337": {"so_4234120_4235537_3": {"section_id": 5902, "quality": 0.8, "length": 4}, "so_4234120_4235537_7": {"section_id": 5911, "quality": 0.7407407407407407, "length": 20}, "so_4234120_4235537_5": {"section_id": 5909, "quality": 0.6923076923076923, "length": 18}}, "n4659": {"so_4234120_4235537_3": {"section_id": 7635, "quality": 0.8, "length": 4}, "so_4234120_4235537_7": {"section_id": 7644, "quality": 0.7407407407407407, "length": 20}, "so_4234120_4235537_5": {"section_id": 7642, "quality": 0.6923076923076923, "length": 18}}}, "4234139": {"ParentId": "4234120", "PostTypeId": "2", "CommentCount": "6", "Body": "<p>Some embedded processors only have a \"shift-by-one\" instruction.  On such processors, the compiler would change <code>x &lt;&lt; 3</code> into <code>((x &lt;&lt; 1) &lt;&lt; 1) &lt;&lt; 1</code>.</p>\n<p>I think the Motorola MC68HCxx was one of the more popular families with this limitation.  Fortunately, such architectures are now quite rare, most now include a barrel shifter with a variable shift size.</p>\n<p>The Intel 8051, which has many modern derivatives, also cannot shift an arbitrary number of bits.</p>\n", "OwnerUserId": "103167", "LastEditorUserId": "103167", "LastEditDate": "2010-11-20T21:17:52.047", "Id": "4234139", "Score": "61", "CreationDate": "2010-11-20T17:58:19.737", "LastActivityDate": "2010-11-20T21:17:52.047"}});