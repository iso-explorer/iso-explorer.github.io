post_cb({"11151886": {"ParentId": "11151687", "CommentCount": "2", "Body": "<p>A <code>vector</code> might be the wrong data structure for you. It requires storage in a single block of memory, which is limited by the size of <code>size_t</code>. This you can increase by compiling for 64 bit systems, but then you can't run on 32 bit systems which might be a requirement.</p>\n<p>If you don't need <code>vector</code>'s particular characteristics (particularly O(1) lookup and contiguous memory layout), another structure such as a <code>std::list</code> might suit you, which has no size limits except what the computer can physically handle as it's a linked list instead of a conveniently-wrapped array.</p>\n", "OwnerUserId": "241544", "PostTypeId": "2", "Id": "11151886", "Score": "0", "CreationDate": "2012-06-22T07:26:07.620", "LastActivityDate": "2012-06-22T07:26:07.620"}, "11152454": {"ParentId": "11151687", "PostTypeId": "2", "CommentCount": "5", "Body": "<p>There are a number of things to say.</p>\n<p><strong>First</strong>, about <strong>the size of <code>std::size_t</code></strong> on 32-bit systems and 64-bit systems, respectively. This is what the standard says about <code>std::size_t</code> (\u00a718.2/6,7):</p>\n<blockquote>\n<p id=\"so_11151687_11152454_0\">6 The type <code>size_t</code> is an implementation-de\ufb01ned unsigned integer type that is large enough to contain the size\n  in bytes of any object.</p>\n<p id=\"so_11151687_11152454_1\">7 [ Note: It is recommended that implementations choose types for <code>ptrdiff_t</code> and <code>size_t</code> whose integer\n  conversion ranks (4.13) are no greater than that of <code>signed long int</code> unless a larger size is necessary to\n  contain all the possible values. \u2014 end note ]</p>\n</blockquote>\n<p>From this it follows that <code>std::size_t</code> will be <em>at least</em> 32 bits in size on a 32-bit system, and <em>at least</em> 64 bits on a 64-bit system. It could be larger, but that would obviously not make any sense.</p>\n<p><strong>Second</strong>, about the idea of <strong>type casting</strong>: For this to work, <strong>even in theory</strong>, you would have to cast (or rather: <em>redefine</em>) the type inside the implementation of <code>std::vector</code> itself, wherever it occurs.</p>\n<p><strong>Third</strong>, when you say you need this super-large vector \"in 32 bits\", does that mean you want to use it on a 32-bit system? In that case, as the others have pointed out already, what you want is impossible, because a 32-bit system simply doesn't have that much memory.</p>\n<p>But, <strong>fourth</strong>, if what you want is to run your program on a 64-bit machine, and use only a 32-bit data type to refer to the <em>number</em> of elements, but possibly a 64-bit type to refer to the total size in bytes, then <code>std::size_t</code> is not relevant because that is used to refer to the total number of elements, and the index of individual elements, but not the size in bytes.</p>\n<p><strong>Finally</strong>, if you are on a 64-bit system and want to use something of extreme proportions that works like a <code>std::vector</code>, that is certainly possible. Systems with 32 GB, 64 GB, or even 1 TB of main memory are perhaps not extremely common, but definitely available.</p>\n<p><strong>However</strong>, to implement such a data type, it is <strong>generally not a good idea</strong> to simply allocate gigabytes of memory <strong>in one contiguous block</strong> (which is what a <code>std::vector</code> does), because of reasons like the following:</p>\n<ul>\n<li>Unless the total size of the vector is determined once and for all at initialization time, the vector will be resized, and quite likely re-allocated, possibly many times as you add elements. Re-allocating an extremely large vector can be a time-consuming operation. <strong>[</strong> I have added this item as an <strong>edit</strong> to my original answer. <strong>]</strong></li>\n<li>The OS will have difficulties providing such a large portion of unfragmented memory, as other processes running in parallel require memory, too. [<strong>Edit:</strong> As correctly pointed out in the comments, this isn't really an issue on any standard OS in use today.]</li>\n<li>On very large servers you also have tens of CPUs and typically NUMA-type memory architectures, where it is clearly preferable to work with relatively smaller chunks of memory, and have multiple threads (possibly each running on a different core) access various chunks of the vector in parallel.</li>\n</ul>\n<p><strong>Conclusions</strong></p>\n<p><strong>A)</strong> If you are on a 32-bit system and want to use a vector that large, using disk-based methods such as the one suggested by @JanHudec is the <strong>only thing that is feasible</strong>.</p>\n<p><strong>B)</strong> If you have access to a large 64-bit system with tens or hundreds of GB, you should look into an implementation that <strong>divides the entire memory area into chunks</strong>. Essentially something that works like a <code>std::vector&lt;std::vector&lt;T&gt;&gt;</code>, where each nested vector represents one chunk. If all chunks are full, you append a new chunk, etc. It is straight-forward to implement an iterator type for this, too. Of course, if you want to optimize this further to take advantage of multi-threading and NUMA features, it will get increasingly complex, but that is unavoidable.</p>\n", "OwnerUserId": "777186", "LastEditorUserId": "777186", "LastEditDate": "2012-06-22T11:17:19.817", "Id": "11152454", "Score": "3", "CreationDate": "2012-06-22T08:12:05.160", "LastActivityDate": "2012-06-22T11:17:19.817"}, "11151819": {"ParentId": "11151687", "PostTypeId": "2", "CommentCount": "5", "Body": "<p><code>size_t</code> is a type that can hold size of any allocable chunk of memory. It follows that you can't allocate more memory than what fits in your <code>size_t</code> and thus can't store more elements in any way.</p>\n<p>Compiling in 64-bits will allow it, but realize that the array still needs to fit in memory. 2<sup>32</sup> is 4 billion, so you are going to go over 4 * sizeof(element) GiB of memory. More than 8 GiB of RAM is still rare, so that does not look reasonable.</p>\n<p>I suggest replacing the vector with the one from <a href=\"http://stxxl.sourceforge.net/\" rel=\"nofollow noreferrer\">STXXL</a>. It uses external storage, so your vector is not limited by amount of RAM. The library claims to handle terabytes of data easily.</p>\n<p>(edit) Pedantic note: <code>size_t</code> needs to hold size of maximal single object, not necessarily size of all available memory. In segmented memory models it only needs to accommodate the offset when each object has to live in single segment, but with different segments more memory may be accessible. It is even possible to use it on x86 with PAE, the \"long\" memory model. However I've not seen anybody actually use it.</p>\n", "OwnerUserId": "201725", "LastEditorUserId": "201725", "LastEditDate": "2012-06-22T12:35:49.763", "Id": "11151819", "Score": "6", "CreationDate": "2012-06-22T07:21:59.223", "LastActivityDate": "2012-06-22T12:35:49.763"}, "11151687": {"CommentCount": "11", "AcceptedAnswerId": "11151819", "PostTypeId": "1", "LastEditorUserId": "777186", "CreationDate": "2012-06-22T07:12:37.203", "LastActivityDate": "2012-06-22T12:35:49.763", "LastEditDate": "2012-06-22T08:53:02.350", "ViewCount": "565", "FavoriteCount": "1", "Title": "Casting size_t to allow more elements in a std::vector", "Id": "11151687", "Score": "2", "Body": "<p>I need to store a huge number of elements in a <code>std::vector</code> (more that the 2^32-1 allowed by unsigned int) in 32 bits. As far as I know this quantity is limited by the <code>std::size_t</code> unsigned int type. May I change this <code>std::size_t</code> by casting to an <code>unsigned long</code>? Would it resolve the problem?</p>\n<p>If that's not possible, suppose I compile in 64 bits. Would that solve the problem without any modification?</p>\n", "Tags": "<c++>", "OwnerUserId": "1474074", "AnswerCount": "3"}, "bq_ids": {"n4140": {"so_11151687_11152454_1": {"section_id": 6709, "quality": 0.8461538461538461, "length": 22}, "so_11151687_11152454_0": {"section_id": 6708, "quality": 0.8461538461538461, "length": 11}}, "n3337": {"so_11151687_11152454_1": {"section_id": 6464, "quality": 0.8461538461538461, "length": 22}, "so_11151687_11152454_0": {"section_id": 6463, "quality": 0.8461538461538461, "length": 11}}, "n4659": {"so_11151687_11152454_1": {"section_id": 8184, "quality": 0.8461538461538461, "length": 22}, "so_11151687_11152454_0": {"section_id": 8183, "quality": 0.8461538461538461, "length": 11}}}});