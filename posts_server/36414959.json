post_cb({"bq_ids": {"n4140": {"so_36414959_36416452_0": {"section_id": 6163, "quality": 0.6428571428571429, "length": 9}, "so_36414959_36416452_1": {"section_id": 5837, "quality": 0.8431372549019608, "length": 43}}, "n3337": {"so_36414959_36416452_0": {"section_id": 5924, "quality": 0.6428571428571429, "length": 9}, "so_36414959_36416452_1": {"section_id": 5607, "quality": 0.8431372549019608, "length": 43}}, "n4659": {"so_36414959_36416452_0": {"section_id": 7660, "quality": 0.6428571428571429, "length": 9}, "so_36414959_36416452_1": {"section_id": 7299, "quality": 0.8431372549019608, "length": 43}}}, "36414959": {"CommentCount": "8", "AcceptedAnswerId": "36415365", "PostTypeId": "1", "LastEditorUserId": "444991", "CreationDate": "2016-04-05T00:07:04.393", "LastActivityDate": "2016-04-11T22:54:02.450", "LastEditDate": "2016-04-09T15:12:28.140", "ViewCount": "5298", "FavoriteCount": "10", "Title": "Why do C++ optimizers have problems with these temporary variables or rather why `v[]` should be avoided in tight loops?", "Id": "36414959", "Score": "61", "Body": "<p>In this <a href=\"http://rextester.com/PQJL94337\">code snippet</a>, I'm comparing performance of two functionally identical loops:</p>\n<pre class=\"lang-c++ prettyprint-override\"><code>for (int i = 1; i &lt; v.size()-1; ++i) {\n  int a = v[i-1];\n  int b = v[i];\n  int c = v[i+1];\n\n  if (a &lt; b  &amp;&amp;  b &lt; c)\n    ++n;\n}\n</code></pre>\n<p>and</p>\n<pre class=\"lang-c++ prettyprint-override\"><code>for (int i = 1; i &lt; v.size()-1; ++i) \n  if (v[i-1] &lt; v[i]  &amp;&amp;  v[i] &lt; v[i+1])\n    ++n;\n</code></pre>\n<p>The first one runs significantly slower than the second one across a number of different C++ compilers with optimization flag set to <code>O2</code>:</p>\n<ul>\n<li>second loop is about 330% <strong>slower</strong> now with Clang 3.7.0</li>\n<li>second loop is about 2% slower with gcc 4.9.3</li>\n<li>second loop is about 2% slower with Visual C++ 2015</li>\n</ul>\n<p>I'm puzzled that modern C++ optimizers have problems handling this case. Any clues why? Do I have to write ugly code without using temporary variables in order to get the best performance?</p>\n<p>Using temporary variables makes the code faster, sometimes dramatically, now. What is going on?</p>\n<p>The full code I'm using is provided below:</p>\n<pre class=\"lang-c++ prettyprint-override\"><code>#include &lt;algorithm&gt;\n#include &lt;chrono&gt;\n#include &lt;random&gt;\n#include &lt;iomanip&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nusing namespace std;\nusing namespace std::chrono;\n\nvector&lt;int&gt; v(1'000'000);\n\nint f0()\n{\n  int n = 0;\n\n  for (int i = 1; i &lt; v.size()-1; ++i) {\n    int a = v[i-1];\n    int b = v[i];\n    int c = v[i+1];\n\n    if (a &lt; b  &amp;&amp;  b &lt; c)\n      ++n;\n  }\n\n  return n;\n}\n\n\nint f1()\n{\n  int n = 0;\n\n  for (int i = 1; i &lt; v.size()-1; ++i) \n    if (v[i-1] &lt; v[i]  &amp;&amp;  v[i] &lt; v[i+1])\n      ++n;\n\n  return n;\n}\n\n\nint main()\n{\n  auto benchmark = [](int (*f)()) {\n    const int N = 100;\n\n    volatile long long result = 0;\n    vector&lt;long long&gt;  timings(N);\n\n    for (int i = 0; i &lt; N; ++i) {\n      auto t0 = high_resolution_clock::now(); \n      result += f();\n      auto t1 = high_resolution_clock::now(); \n\n      timings[i] = duration_cast&lt;nanoseconds&gt;(t1-t0).count();\n    }\n\n    sort(timings.begin(), timings.end());\n    cout &lt;&lt; fixed &lt;&lt; setprecision(6) &lt;&lt; timings.front()/1'000'000.0 &lt;&lt; \"ms min\\n\";\n    cout &lt;&lt; timings[timings.size()/2]/1'000'000.0 &lt;&lt; \"ms median\\n\" &lt;&lt; \"Result: \" &lt;&lt; result/N &lt;&lt; \"\\n\\n\";\n  };\n\n  mt19937                    generator   (31415);   // deterministic seed\n  uniform_int_distribution&lt;&gt; distribution(0, 1023);\n\n  for (auto&amp; e: v) \n    e = distribution(generator);\n\n  benchmark(f0);\n  benchmark(f1);\n\n  cout &lt;&lt; \"\\ndone\\n\";\n\n  return 0;\n}\n</code></pre>\n", "Tags": "<c++><performance><optimization>", "OwnerUserId": "219153", "AnswerCount": "4"}, "36421769": {"ParentId": "36414959", "PostTypeId": "2", "CommentCount": "3", "Body": "<p>f0 and f1 are semantically different.</p>\n<p><code>x() &amp;&amp; y()</code> involves a short-circuit in the case of x() being false as we know. This means that if x() is false , then y() <em>must not</em> be evaluated.</p>\n<p>This prevents prefetching of the data in order to evaluate y() and (at least on clang) is causing the insertion of a conditional jump, which is resulting in branch-predictor misses.</p>\n<p>Adding another 2 tests proves the point.</p>\n<pre><code>#include &lt;algorithm&gt;\n#include &lt;chrono&gt;\n#include &lt;random&gt;\n#include &lt;iomanip&gt;\n#include &lt;iostream&gt;\n#include &lt;vector&gt;\n\nusing namespace std;\nusing namespace std::chrono;\n\nvector&lt;int&gt; v(1'000'000);\n\nint f0()\n{\n    int n = 0;\n\n    for (int i = 1; i &lt; v.size()-1; ++i) {\n        int a = v[i-1];\n        int b = v[i];\n        int c = v[i+1];\n\n        if (a &lt; b  &amp;&amp;  b &lt; c)\n            ++n;\n    }\n\n    return n;\n}\n\n\nint f1()\n{\n    int n = 0;\n\n    auto s = v.size() - 1;\n    for (size_t i = 1; i &lt; s; ++i)\n        if (v[i-1] &lt; v[i]  &amp;&amp;  v[i] &lt; v[i+1])\n            ++n;\n\n    return n;\n}\n\nint f2()\n{\n    int n = 0;\n\n    auto s = v.size() - 1;\n    for (size_t i = 1; i &lt; s; ++i)\n    {\n        auto t1 = v[i-1] &lt; v[i];\n        auto t2 = v[i] &lt; v[i+1];\n        if (t1 &amp;&amp; t2)\n            ++n;\n    }\n\n    return n;\n}\n\nint f3()\n{\n    int n = 0;\n\n    auto s = v.size() - 1;\n    for (size_t i = 1; i &lt; s; ++i)\n    {\n        n += 1 * (v[i-1] &lt; v[i]) * (v[i] &lt; v[i+1]);\n    }\n\n    return n;\n}\n\n\n\nint main()\n{\n    auto benchmark = [](int (*f)()) {\n        const int N = 100;\n\n        volatile long long result = 0;\n        vector&lt;long long&gt;  timings(N);\n\n        for (int i = 0; i &lt; N; ++i) {\n            auto t0 = high_resolution_clock::now();\n            result += f();\n            auto t1 = high_resolution_clock::now();\n\n            timings[i] = duration_cast&lt;nanoseconds&gt;(t1-t0).count();\n        }\n\n        sort(timings.begin(), timings.end());\n        cout &lt;&lt; fixed &lt;&lt; setprecision(6) &lt;&lt; timings.front()/1'000'000.0 &lt;&lt; \"ms min\\n\";\n        cout &lt;&lt; timings[timings.size()/2]/1'000'000.0 &lt;&lt; \"ms median\\n\" &lt;&lt; \"Result: \" &lt;&lt; result/N &lt;&lt; \"\\n\\n\";\n    };\n\n    mt19937                    generator   (31415);   // deterministic seed\n    uniform_int_distribution&lt;&gt; distribution(0, 1023);\n\n    for (auto&amp; e: v) \n        e = distribution(generator);\n\n    benchmark(f0);\n    benchmark(f1);\n    benchmark(f2);\n    benchmark(f3);\n\n    cout &lt;&lt; \"\\ndone\\n\";\n\n    return 0;\n}\n</code></pre>\n<p>results (apple clang, -O2):</p>\n<pre><code>1.233948ms min\n1.320545ms median\nResult: 166850\n\n3.366751ms min\n3.493069ms median\nResult: 166850\n\n1.261948ms min\n1.361748ms median\nResult: 166850\n\n1.251434ms min\n1.353653ms median\nResult: 166850\n</code></pre>\n", "OwnerUserId": "2015579", "LastEditorUserId": "2015579", "LastEditDate": "2016-04-05T10:42:41.850", "Id": "36421769", "Score": "7", "CreationDate": "2016-04-05T09:04:50.007", "LastActivityDate": "2016-04-05T10:42:41.850"}, "36415365": {"ParentId": "36414959", "PostTypeId": "2", "CommentCount": "10", "Body": "<p>It seems like the compiler lacks knowledge about the relationship between <code>std::vector&lt;&gt;::size()</code> and internal vector buffer size. Consider <code>std::vector</code> being our custom <code>bugged_vector</code> vector-like object with slight bug - its <code>::size()</code> can sometimes be one more than internal buffer size <code>n</code>, but only then <code>v[n-2] &gt;= v[n-1]</code>.</p>\n<p>Then two snippets have different semantics again: first one has undefined behavior, as we access element <code>v[v.size() - 1]</code>. The second one, however, doesn't have: due to short-circuit nature of <code>&amp;&amp;</code>, we don't ever read <code>v[v.size() - 1]</code> on the last iteration.</p>\n<p>So, if compiler can't prove that our <code>v</code> is not a <code>bugged_vector</code>, it must short-circuit, which introduce additional jump in a machine code.</p>\n<p>By looking at assembly output from <code>clang</code>, we can see that it actually happens.</p>\n<p>From <a href=\"http://gcc.godbolt.org/#compilers:!((compiler:clang37x,options:'-Wall+-Wextra+-std%3Dgnu%2B%2B14+-fverbose-asm+-O2+-mtune%3Dgeneric',sourcez:MQSwdgxgNgrgJgUwAQB4CGUDmB7ATiAFwAsBbAPgChRJZFUIjdsxtLrp5kVc0w5tyVcBzooQA3iAAObYbS7iAzgVwI0g9vNQA3BBAJ5KFGIvCYkYdQkVS0EZMrgBuY6bDnLJa7ftJHALn8GJhYXCl19PDEwAjIkbQAKAEYAcgAGDPSMgEow8AIkADM0hOyKAG8KJCR8iyQAXiQ0sOrCvCQE2pAGpCSnGtR4gDpTAC8EUoBaPqQAalmQbKRK6uratB7tAG0QaYBdF1WamKQAI02dg6rV2ogLkFmkq%2Bu1wo6NlDPqgDJv6vPPhAykdqvMwIckABfCjXVQEGC4MAWFzQmG1QpJUoVa61JGNZow1rtTonbqNGbdT7aEYgcZTGbzRZIF41N4Jba7J6DDl7H5/eKXbk7R57YEgsEtJBwhFI8EUVEUWokNDgLErJBoGAGM4ISCkNC4ADWPS2exJBQSACpCtlSkt1dUIMxlMcCgA5HpJDKS6rabBQNAEEBQZBQZjmMPuKXWGBQAr4iG%2BvQGXAoSMR8NxJBBkhmRQJN25QmrNq4DpdHrNAafN39Rn2lnVTXaghpHpEECYIgAfVUin9WvEYG70GwEENgRYAHdSv1G9HFLGCrNGoVZ/Pm9hs0l252e32B0HmCOw%2BPJ9gZ7lmfOc3nBY04AjA0OR2hlChLCxFHpmHBFGQEgIJJJlbbIhidGAYnXI5USOftcAIQCQFzdxFCGU4EEwVVsgAGmzZC8yGXU4DtRMkAggoUE%2BQoQAADwQOBUE%2Bb8CCkVQIBAUxmASAA2JYqPwlDMDQwoQkQ7IAHpUgyNIsjSIY2wEgAiEhFCQFCAB0wCUsiKKYwS71vVCaTpSSACY9ikuS5IU/SVLUrw4BAXgtKUuyACUYzjfwkDcgS%2ByXCSPWUrTXIhSFJRIICAE5ooAZgAdhBZKUuqTBdQQHgU1WBI4qSAAWJIAFYr2qCSJKQRACEylDOKDO5vwY65IJAUsSG7fJuyc5R8FOQdmBQOJupUEA%2BqPMAEjSPCvTMuKi2uUt3i1bA/gQHztCWFlkAfOrev6ib0rATLAzweb/l1BhlSNBJiiLc69Suw0bqSM7yOwLU7K0/gjrC4tpURJoUSAAAA%3D)),filterAsm:(commentOnly:!t,directives:!t,intel:!t,labels:!t),version:3\" rel=\"nofollow noreferrer\">the Godbolt Compiler Explorer</a>, with clang 3.7.0 -O2, the loop in <code>f0</code> is:</p>\n<pre><code>### f0: just the loop\n.LBB1_2:                                # =&gt;This Inner Loop Header: Depth=1\n    mov     edi, ecx\n    cmp     edx, edi\n    setl    r10b\n    mov     ecx, dword ptr [r8 + 4*rsi + 4]\n    lea     rsi, [rsi + 1]\n    cmp     edi, ecx\n    setl    dl\n    and     dl, r10b\n    movzx   edx, dl\n    add     eax, edx\n    cmp     rsi, r9\n    mov     edx, edi\n    jb      .LBB1_2\n</code></pre>\n<p>And for <code>f1</code>:</p>\n<pre><code>### f1: just the loop\n.LBB2_2:                                # =&gt;This Inner Loop Header: Depth=1\n    mov     esi, r10d\n    mov     r10d, dword ptr [r9 + 4*rdi]\n    lea     rcx, [rdi + 1]\n    cmp     esi, r10d\n    jge     .LBB2_4                     # &lt;== This is Extra Jump\n    cmp     r10d, dword ptr [r9 + 4*rdi + 4]\n    setl    dl\n    movzx   edx, dl\n    add     eax, edx\n.LBB2_4:                                # %._crit_edge.3\n    cmp     rcx, r8\n    mov     rdi, rcx\n    jb      .LBB2_2\n</code></pre>\n<p>I've pointed out the extra jump in <code>f1</code>. And as we (hopefuly) know, conditional jumps in a tight loops are bad for performance.  (See the performance guides in the <a class=\"post-tag\" href=\"/questions/tagged/x86\" rel=\"tag\" title=\"show questions tagged 'x86'\">x86</a> tag wiki for details.)</p>\n<p><s>GCC and Visual Studio are aware that <code>std::vector</code> is well-behaved, and produce almost identical assembly for both snippets.</s>\n<strong>Edit</strong>. It turns out <code>clang</code> does better job optimizing the code. All three compilers can't prove that it is safe to read <code>v[i + 1]</code> prior to comparison in the second example (or choose not to), but only <code>clang</code> manages to optimize the first example with the additional information that reading <code>v[i + 1]</code> is either valid or UB.</p>\n<p>A performance difference of 2% is negligible can be explained by different order or choice of some instructions.</p>\n", "OwnerUserId": "2512323", "LastEditorUserId": "444991", "LastEditDate": "2016-04-09T15:09:40.450", "Id": "36415365", "Score": "50", "CreationDate": "2016-04-05T00:57:47.587", "LastActivityDate": "2016-04-09T15:09:40.450"}, "36416452": {"CommentCount": "23", "CreationDate": "2016-04-05T03:08:55.340", "CommunityOwnedDate": "2016-04-05T05:19:10.497", "LastEditorUserId": "-1", "LastActivityDate": "2016-04-05T11:50:31.497", "ParentId": "36414959", "PostTypeId": "2", "LastEditDate": "2017-05-23T12:09:39.513", "Id": "36416452", "Score": "41", "Body": "<p>Here's additional insight to expand on @deniss' answer, which correctly diagnosed the issue.</p>\n<p>Incidentally, this is related to <a href=\"https://stackoverflow.com/questions/11227809/why-is-processing-a-sorted-array-faster-than-an-unsorted-array\">the most popular C++ Q&amp;A of all time <em>\"Why is processing a sorted array faster than an unsorted array?\"</em></a>.</p>\n<p>The main issue is the compiler must honor the logical AND operator (&amp;&amp;) and not load from v[i+1] unless the first condition is true.  This is a consequence of the semantics of the Logical AND operator as well as the tightened memory model semantics introduced with C++11, the relevant clauses in the draft of the standard are</p>\n<blockquote>\n<h3>5.14 Logical AND operator [expr.log.and]</h3>\n<p id=\"so_36414959_36416452_0\">Unlike <strong>&amp;</strong>, <strong>&amp;&amp;</strong> guarantees left-to-right evaluation: the second\n  operand is not evaluated if the first operand is <strong>false</strong>.<br><sub><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3797.pdf\" rel=\"nofollow noreferrer\">ISO C++14 Standard (draft N3797)</a></sub></br></p>\n</blockquote>\n<p>and for speculative reads</p>\n<blockquote>\n<h3>1.10 Multi-threaded executions and data races [intro.multithread]</h3>\n<p id=\"so_36414959_36416452_1\"><em>23</em> [ <em>Note:</em> Transformations that introduce a speculative read of a potentially shared memory location may not preserve the semantics of the C++ program as defined in this standard, since they potentially introduce a data race. However, they are typically valid in the context of an optimizing compiler that targets a specific machine with well-defined semantics for data races. They would be invalid for a hypothetical machine that is not tolerant of races or provides hardware race detection. \u2014 <em>end note</em> ]<br><sub><a href=\"http://www.open-std.org/jtc1/sc22/wg21/docs/papers/2013/n3797.pdf\" rel=\"nofollow noreferrer\">ISO C++14 Standard (draft N3797)</a></sub></br></p>\n</blockquote>\n<p>My guess is optimizers play it safe and currently choose not to issue speculative loads to potentially shared memory rather than special case for each target processor whether the speculative load could introduce a detectable data race for that target.</p>\n<p>In order to implement this, the compiler generates a conditional branch.  Usually this isn't noticeable because modern processors have very sophisticated branch prediction, and the misprediction rate is typically very low.  However the data here is random - this kills branch prediction.  The cost of a misprediction is 10 to 20 CPU cycles, considering that the CPU is typically retiring 2 instructions per cycle this is equivalent to 20 to 40 instructions.  If the prediction rate is 50% (random) then every iteration has a mispredict penalty equivalent to 10 to 20 instructions - <strong>HUGE</strong>.</p>\n<p><strong>Note:</strong> The compiler could prove that elements <code>v[0]</code> to <code>v[v.size()-2]</code> will be referenced, in that order, regardless of the values they contain.  This would allow the compiler in this case to generate code that unconditionally loads all but the last element of the vector. The last element of the vector, at v[v.size()-1], may only be loaded in the last iteration of the loop and only if the first condition is true.\nThe compiler could therefore generate code for the loop without the short circuit branch up until the last iteration, then use different code with the short circuit branch for the last iteration - that would require the compiler knowing that the data is random and branch prediction is useless and therefore that it is worth bothering with that - compilers aren't that sophisticated - yet.</p>\n<p>To avoid the conditional branch generated by the Logical AND (&amp;&amp;) and avoid loading the memory locations into local variables we can change the Logical AND operator into a Bitwise AND, <a href=\"http://rextester.com/WBUJI37422\" rel=\"nofollow noreferrer\">code snippet here</a>, the result is almost 4x faster when the data is random</p>\n<pre><code>int f2()\n{\n  int n = 0;\n\n  for (int i = 1; i &lt; v.size()-1; ++i) \n     n += (v[i-1] &lt; v[i])  &amp;  (v[i] &lt; v[i+1]); // Bitwise AND\n\n  return n;\n}\n</code></pre>\n<p>Output</p>\n<pre><code>3.642443ms min\n3.779982ms median\nResult: 166634\n\n3.725968ms min\n3.870808ms median\nResult: 166634\n\n1.052786ms min\n1.081085ms median\nResult: 166634\n\n\ndone\n</code></pre>\n<p>The result on gcc 5.3 is 8x faster (<a href=\"http://coliru.stacked-crooked.com/a/e3893f8bcd024d89\" rel=\"nofollow noreferrer\">live in Coliru here</a>)</p>\n<pre><code>g++ --version\ng++ -std=c++14  -O3 -Wall -Wextra -pedantic -pthread -pedantic-errors main.cpp -lm  &amp;&amp; ./a.out\ng++ (GCC) 5.3.0\nCopyright (C) 2015 Free Software Foundation, Inc.\nThis is free software; see the source for copying conditions.  There is NO\nwarranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n\n3.761290ms min\n4.025739ms median\nResult: 166634\n\n3.823133ms min\n4.050742ms median\nResult: 166634\n\n0.459393ms min\n0.505011ms median\nResult: 166634\n\n\ndone\n</code></pre>\n<p>You might wonder how the compiler can evaluate the comparison <code>v[i-1] &lt; v[i]</code> <em>without</em> generating a conditional branch.  The answer depends on the target, for x86 this is possible because of the <code>SETcc</code> instruction, which generates a one byte result, 0 or 1, depending on a condition in the EFLAGS register, the same condition that could be used in a conditional branch, but without branching.  In the generated code given by @deniss you can see <code>setl</code> generated, that sets the result to 1 if the condition \"less than\" is met, which is evaluated by the previous compare instruction:</p>\n<pre><code>cmp     edx, edi       ; a &lt; b ?\nsetl    r10b           ; r10b = a &lt; b ? 1 : 0\nmov     ecx, dword ptr [r8 + 4*rsi + 4] ; c = v[i+1]\nlea     rsi, [rsi + 1] ; ++i\ncmp     edi, ecx       ; b &lt; c ?\nsetl    dl             ; dl = b &lt; c ? 1 : 0\nand     dl, r10b       ; dl &amp;= r10b\nmovzx   edx, dl        ; edx = zero extended dl\nadd     eax, edx       ; n += edx\n</code></pre>\n", "OwnerUserId": "521897"}, "36559913": {"ParentId": "36414959", "PostTypeId": "2", "CommentCount": "0", "Body": "<p>None of the answers so far have given a version of <code>f()</code> that gcc or clang can fully optimize.  They all generate asm that does both compares each iteration.  See <a href=\"http://gcc.godbolt.org/#compilers:!((compiler:clang380,options:'-Wall+-Wextra+-std%3Dgnu%2B%2B14+-fverbose-asm+-O3+-march%3Dcore2',sourcez:MQSwdgxgNgrgJgUwAQB4CGUDmB7ATiAFwAsBbAPgChRJZFUIjdsxtLrp5kVc0w5tyVcBzooQA3iAAObYbS7iAzgVwI0g9vNQA3BBAJ5KFGIvCYkYdQkVS0EZMrgBuY6bDnLJa7ftJHALn8GJhYXCl19PDEwAjIkbQAKAEYABjT0gEow8AIkADMAFgSMpABvCiQkHIskAF4kFLDK0wAvBAB9XJgpKQRcOviAOlaEYoBaJJdKvLwkBJHOqoHJpZQkbt7cJyQAah2QEvLK46qYpDQB7QBtEAmAXQAaJAAjS5vHpAg3kB2ku6mTi9sNgoOdnu0oLl6hc1s8ASdnsDQc8IBCoS9UJ8moCwLt6gk0ODIUgAGQvVGQrKVAD01L8AmQqgwUAAnkgSOBZhBsIgxpgELi4CA8nk%2BgL7Ip8kwSPkAEz%2Bc4wAxjCIGfBtSUGJAAKkUUBAmCIBFZ2vyaGUfU%2BPOQM36AGE8AhZRUkABfCgu1QEGC4XFgFzuijVPIAZmKZRd1Vx9UakbOF3q1xSH1eiaufycAEhaUgACpEazIIhoHoCl4IW3IBh6ADWZiGIyeiFFaAI9eIyCgwKk8T0apAbX6s0UDOIZhdiJBYLRAxhL2xCy6pf6ieGA9GGQmANtc0XS3qKxAmI2fW2ewOEcBp1yXzTPwzmezdOYrKQzE72DQcHOfCtJFsqinKYdAdkgXbYFILoIki5IzvUrxrBA2InLiOz4oSM5kiiaJZB6V4YcS8EUgQ8LHAm85QccqZYi67qVF6Pp%2BgGHpBmceSyuGRzXjUMYAnuJ4rg267jJMT55kQICSrw357keaCAaBL5sgwvD8pKeTSvkKRPKoyhVJKABWJg3gIPi5LaJCtm27guju8zrosR4HtsR5rAJZ77IclHVOR1y3H8TzUX5Hy3vENy/P83l5HMc6vKSZIYohGSUZUez%2BrRnoIN6voWMxHrBiknFxrk0YNNuswJNUTlIIemLaGubQiR5F5cZUPnfPcgXfCF3wRaRwoxZicUkglCGfMlV5pQCdFIAxOXpYGrHmUkRVtWcpWxtMFWycsLl1Q1G5brsnmXscA0JH59x1e8lQjZUwXXfedwTYCU0ZfRWWMblFCLaA0UpEINCcKgIAkByMT4GAgxEJQwYAMpwwAopxNJ0gAqgAcgAYgAkhjONwwAEojAAieFVNFF0HeGawABwvR92W4qGxTYu07QkEksq00eBgEBgAwcyQ7SKFlg7YCLIBc7TrMui63JgHp1Taj29QkkmkVbf0q3HDmCC6LgbKEH0rbiLiUhMHAMASuc%2BQwFAoKqrM2DRbp9sEOpmnEKoyDgFISq9pEuCKJRObEK25yAS7eSi7kzxsu%2BSAIFACBeGc4BJ4QBb9EKqj6GbUoCEgoEAEQQD6qgxCXlHs5z3NHto7QW/rgtgxCn5W5L0sJHMCS19LIDahkCRSBMGQlLhJz9/X8TtGACAAB7okL7dfjAXfcz3CR9xzA9DyPvzj0guE17vM%2BN%2BXuCCSvXZrxvMu99PPP71IOwpEfk%2B69ST8N%2B0l%2Bt8LDABowC4HaAgKQIAZaN3nkvJ4jdm7aCeAUKkSAcwsFyAARxgMbWaBojQKhAIgAWth9SYBAacf2uRJIWEXlQggJs2zMAAOSSgAAYINYYHNUlFT51x5p8f8TdVDaAAX/f8mACBgIgSGDiF8K5wKEfrT%2BlQf4CKkH/CuoiIDiMkeAkAMiLpz1oQoy%2BuAT5TzPvw7R6iEHtGkhoq%2Boj7GmG7tYhxuAnhuIQeY44fMBb1BXooGA4I9EGL8VATxgjbH2NMSg1BdIgnPBUHYXIExKi1DiF%2Bb8SRyaozUYokR9Q3GmKzPEyoABZNALJnjIBIcA/opiBR0IYWbFhf4ALIAAO5ZzAuaZpPBGFgDadyf88kEAAH4MrFR4kgIgQThahI4uEz%2BOZFAQAwPJT4KdeDdDfNFKAfSk4pzTgQSZmUmbfXdMAAUQo8gsWqJZcAxQKBcTQEqbA5ZICkHkjWAYVw7iVTOAkbUeRh5H1alaRWVCzgY2WGkZC8QQSmxTmBZg5guzuFmtYd2AxNrHCdrgFAGL0VoriMXUGZhFAJAxj4rWcwqq4r2msDGzUvL4XecXFIAwJKGnaLpEESozZ/y7BAGsgQWCdNlleN2xI0L5ClYCN5WoCBJG5Xgvl1gBWDOFdgUV4rsCStpb4il7hFA3XqFbAZQr1nKBQJYFgosFZwEUGQBIKqxgEHfoMbkMAYgKsqIGE4I5cAEDdSazAihBg1MwE8jITw2zgwjYMG5xRlFWgDigNYeQQALwQN%2BTNfgsrNwgJJM2CQABsJQC0JspYMDSzBQ0ZGpEkJh6RW1pEGFygtJcSCSnBgAHTACXUiPrcjVvDWamtprqZNtlHcZt7aUiLs7agNYPa%2B15pALwQdJdV1IBLgAJWxZCBUu6C0yoINSWF3bB07umtiEgKqACcT6QwAHYryfs/fyeeAzZiVASCGJIBQkgAFY4k5kQPQ3A4NJJti%2BKLPNLpfXCjwMLHI7QhTKHwM8QVzAUBxCwyoEAuHBkJG0jVFIsoQw%2BLskq7AZIEAKm0AzJOAwiM4bw2ABIP6GF4B8aOvdJc8gpDvZUGpXzLK4BrAkETn9BPdryEkMTnyGBSZk0p%2BT2AM1rvYipiTamfmydlFpnT%2B7Qz6fFN86TsmaMAgU7pgolnJNGcKCfHM6ax1rsHfweed7yZzVxLGd0QAAA)),filterAsm:(commentOnly:!t,directives:!t,intel:!t,labels:!t),version:3\" rel=\"nofollow\"><strong>the code with asm output on the Godbolt Compiler Explorer</strong></a>.  (Important background knowledge for predicting performance from asm output: <a href=\"http://agner.org/optimize/\" rel=\"nofollow\">Agner Fog's microarchitecture guide</a>, and other links on the <a class=\"post-tag\" href=\"/questions/tagged/x86\" rel=\"tag\" title=\"show questions tagged 'x86'\">x86</a> tag wiki.  As always, it usually works best to profile with performance counters to find stalls.)</p>\n<p><code>v[i-1] &lt; v[i]</code> is work we already did last iteration, when we evaluated <code>v[i] &lt; v[i+1]</code>.  In theory, helping the compiler grok that would let it optimize better (see <code>f3()</code>).  In practice, that ends up defeating auto-vectorization in some cases, and gcc emits code with partial-register stalls, even with <code>-mtune=core2</code> where that's a huge problem.</p>\n<p>Manually hoisting the <code>v.size() - 1</code> out of the loop's upper bound check seems to help.  The OP's <code>f0</code> and <code>f1</code> don't actually re-compute <code>v.size()</code> from the start/end pointers in <code>v</code>, but somehow it still optimizes less well than when computing a <code>size_t upper = v.size() - 1</code> outside the loop (<code>f2()</code> and <code>f4()</code>).</p>\n<p>A separate issue is that using an <code>int</code> loop counter with a <code>size_t</code> upper bound means the loop is potentially infinite.  I'm not sure how much impact this has on other optimizations.</p>\n<hr>\n<p><strong>Bottom line: compilers are complex beasts</strong>.  Predicting which version will optimize well is not at all obvious or straightforward.</p>\n<hr>\n<p>Results on 64bit Ubuntu 15.10, on Core2 E6600 (Merom/Conroe microarchitecture).</p>\n<pre><code>clang++-3.8 -O3 -march=core2   |   g++ 5.2 -O3 -march=core2         | gcc 5.2 -O2 (default -mtune=generic)\nf0    1.825ms min(1.858 med)   |   5.008ms min(5.048 med)           | 5.000 min(5.028 med)\nf1    4.637ms min(4.673 med)   |   4.899ms min(4.952 med)           | 4.894 min(4.931 med)\nf2    1.292ms min(1.323 med)   |   1.058ms min(1.088 med) (autovec) | 4.888 min(4.912 med)\nf3    1.082ms min(1.117 med)   |   2.426ms min(2.458 med)           | 2.420 min(2.465 med)\nf4    1.291ms min(1.341 med)   |   1.022ms min(1.052 med) (autovec) | 2.529 min(2.560 med)\n</code></pre>\n<p>Results would be different on Intel SnB-family hardware, esp. IvyBridge and later where there would be no partial register slowdowns at all.  Core2 is limited by slow unaligned loads, and only one load per cycle.  The loops may be small enough that decode isn't an issue, though.</p>\n<hr>\n<p><code>f0</code> and <code>f1</code>:</p>\n<p>gcc 5.2: The OP's <code>f0</code> and <code>f1</code> both make branchy loops, and won't auto-vectorize.  <code>f0</code> only uses one branch, though, and uses a weird <code>setl sil</code> / <code>cmp sil, 1</code> / <code>sbb eax, -1</code> to do the second half of the short-circuit compare.  So it's still doing both comparisons on every iteration.</p>\n<p>clang 3.8: <code>f0</code>: only one load per iteration, but does both compares and <code>and</code>s them together.  <code>f1</code>: both compares each iteration, one with a branch to preserve the C semantics.  Two loads per iteration.</p>\n<hr>\n<pre><code>int f2() {\n  int n = 0;\n  size_t upper = v.size()-1;   // difference from f0: hoist upper bound and use size_t loop counter\n  for (size_t i = 1; i &lt; upper; ++i) {\n    int a = v[i-1], b = v[i], c = v[i+1];\n    if (a &lt; b  &amp;&amp;  b &lt; c)\n      ++n;\n  }\n  return n;\n}\n</code></pre>\n<p>gcc 5.2 <code>-O3</code>: auto-vectorizes, with three loads to get the three offset vectors needed to produce one vector of 4 compare results.  Also, after combining the results from two <code>pcmpgtd</code> instructions, compares them against a vector of all-zero and then masks that.  Zero is already the identity element for addition, so that's really silly.</p>\n<p>clang 3.8 <code>-O3</code>: unrolls: every iteration does two loads, three cmp/setcc, two <code>and</code>s, and two <code>add</code>s.</p>\n<hr>\n<pre><code>int f4() {\n  int n = 0;\n\n  size_t upper = v.size()-1;\n  for (size_t i = 1; i &lt; upper; ++i) {\n      int a = v[i-1], b = v[i], c = v[i+1];\n      bool ab_lt = a &lt; b;\n      bool bc_lt = b &lt; c;\n\n      n += (ab_lt &amp; bc_lt);  // some really minor code-gen differences from f2: auto-vectorizes to better code that runs slightly faster even for this large problem size\n  }\n\n  return n;\n}\n</code></pre>\n<ul>\n<li>gcc 5.2 <code>-O3</code>: autovectorizes like <code>f2</code>, but without the extra <code>pcmpeqd</code>.</li>\n<li>gcc 5.2 <code>-O2</code>: didn't investigate why this is twice as fast as <code>f2</code>.</li>\n<li>clang <code>-O3</code>: about the same code as <code>f2</code>.</li>\n</ul>\n<hr>\n<h3>Attempt at compiler hand-holding</h3>\n<pre><code>int f3() {\n  int n = 0;\n  int a = v[0], b = v[1];   // These happen before checking v.size, defeating the loop vectorizer or something\n  bool ab_lt = a &lt; b;\n\n  size_t upper = v.size()-1;\n  for (size_t i = 1; i &lt; upper; ++i) {\n      int c = v[i+1];       // only one load and compare inside the loop\n      bool bc_lt = b &lt; c;\n\n      n += (ab_lt &amp; bc_lt);\n\n      ab_lt = bc_lt;\n      a = b;                // unused inside the loop, only the compare result is needed\n      b = c;\n  }\n  return n;\n}\n</code></pre>\n<ul>\n<li><p>clang 3.8 <code>-O3</code>: Unrolls with 4 loads inside the loop (clang typically likes to unroll by 4 when there aren't complex loop-carried dependencies).<br>\n4 cmp/setcc, 4x and/movzx, 4x add.  So clang did exactly what I was hoping, and made near-optimal scalar code.  <strong>This was the fastest non-vectorized version</strong>, and (on core2 where <code>movups</code> unaligned loads are slow) is as fast as gcc's vectorized versions.</br></p></li>\n<li><p>gcc 5.2 <code>-O3</code>:  Fails to auto-vectorize.  My theory on that is that accessing the array outside the loop confuses the auto-vectorizer.  Maybe because we do it before checking <code>v.size()</code>, or maybe just in general.</p>\n<p>Compiles to the scalar code we'd hope for, with one load, one cmp/setcc, and one <code>and</code> per iteration.  But <strong>gcc creates a partial-register stall</strong>, even with <code>-mtune=core2</code> where it's a huge problem (2 to 3 cycle stall to insert a merging uop when reading a wide reg after writing only part of it).  (<code>setcc</code> is only available with an 8-bit operand size, which IMO is something AMD should have changed when they designed the AMD64 ISA.)  It's the main reason why gcc's code runs 2.5x slower than clang's.</p></li>\n</ul>\n<p></p>\n<pre><code>## the loop in f3(), from gcc 5.2 -O3 (same code with -O2)\n.L31:\n    add     rcx, 1    # i,\n    mov     edi, DWORD PTR [r10+rcx*4]        # a, MEM[base: _19, index: i_13, step: 4, offset: 0]\n    cmp     edi, r8d  # a, a                 # gcc's verbose-asm comments are a bit bogus here: one of these `a`s is from the last iteration, so this is really comparing c, b\n    mov     r8d, edi  # a, a\n    setg    sil     #, tmp124\n    and     edx, esi  # D.111089, tmp124     # PARTIAL-REG STALL: reading esi after writing sil\n    movzx   edx, dl                          # using movzx to widen sil to esi would have solved the problem, instead of doing it after the and\n    add     eax, edx  # n, D.111085          # n += ...\n    cmp     r9, rcx   # upper, i\n    mov     edx, esi  # ab_lt, tmp124\n    jne     .L31      #,\n    ret\n</code></pre>\n<hr>\n</hr></hr></hr></hr></hr></hr></hr>", "OwnerUserId": "224132", "LastEditorUserId": "224132", "LastEditDate": "2016-04-11T22:54:02.450", "Id": "36559913", "Score": "2", "CreationDate": "2016-04-11T21:49:03.773", "LastActivityDate": "2016-04-11T22:54:02.450"}});