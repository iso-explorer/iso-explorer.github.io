post_cb({"12704054": {"Id": "12704054", "PostTypeId": "2", "Body": "<p>What you are trying to do is not supported, currently, by the CUDA compiler and runtime (as of CUDA 5.0). Section D.2.6.3 of the CUDA C Programming Guide v5.0 reads:</p>\n<blockquote>\n<h2>D.2.6.3 Virtual Functions</h2>\n<p id=\"so_12701170_12704054_0\">When a function in a derived class overrides a virtual function in a base class, the execution space qualifiers (i.e., <code>__host__</code>, <code>__device__</code>) on the overridden and overriding functions must match.</p>\n<p id=\"so_12701170_12704054_1\">It is not allowed to pass as an argument to a <code>__global__</code> function an object of a class\n  with virtual functions.</p>\n<p id=\"so_12701170_12704054_2\">The virtual function table is placed in global or constant memory by the compiler.</p>\n</blockquote>\n<p>What I recommend is that you encapsulate the data of your class separately from the functionality of the class. For example, store the data in a struct. If you plan to operate on arrays of these objects, store the data in a structure of arrays (for performance -- outside the scope of this question).  Allocate the data structures on the host using <code>cudaMalloc</code>, and then pass the data to the kernel as arguments, rather than passing the class with virtual methods. </p>\n<p>Then construct your objects with virtual methods on the device. The constructor of your class with virtual methods would take the device pointer kernel parameters as arguments. The virtual <strong>device</strong> methods could then operate on the device data.</p>\n<p>The same approach would work to enable allocating the data in one kernel on the device, and accessing it in another kernel on the device (since again, classes with virtual functions can't be parameters to the kernels).</p>\n", "Score": "5", "LastActivityDate": "2012-10-03T07:46:34.663", "CreationDate": "2012-10-03T07:46:34.663", "ParentId": "12701170", "CommentCount": "3", "OwnerUserId": "749748"}, "bq_ids": {"n4140": {"so_12701170_12704054_1": {"length": 5, "quality": 0.5555555555555556, "section_id": 47}}, "n4659": {"so_12701170_12704054_1": {"length": 5, "quality": 0.5555555555555556, "section_id": 48}}}, "12701170": {"ViewCount": "2616", "LastEditDate": "2017-05-23T10:27:18.133", "AcceptedAnswerId": "12704054", "Title": "Cuda virtual class", "CreationDate": "2012-10-03T02:32:04.083", "LastActivityDate": "2012-12-29T19:02:27.960", "CommentCount": "1", "Body": "<p>I would like to execute some virtual methods in a cuda kernel, but instead of creating the object in the same kernel I would like to create it on the host and copy it to gpu memory.</p>\n<p>I am successfully creating objects in a kernel and call a virtual method. The problem arises when copying the object. This makes sense because obviously the virtual function pointer is bogus.\nWhat happens is simply \"Cuda grid launch failed\", at least this is what Nsight says.\nBut when having a look at the SASS it crashes on the dereferencing of the virtual function pointer, which makes sense.</p>\n<p>I am of course using Cuda 4.2 as well as compiling with \"compute_30\" on a fitting card.</p>\n<p>So what is the recommended way to go? Or is this feature simply not supported?</p>\n<p>I had the idea to run a different kernel first which creates dummy objects and extract the virtual function pointer to \"patch\" my objects before copying them. Sadly this is not really working (haven't figured it out yet) as well as it would be an ugly solution.</p>\n<p>P.S. This is actually a rerun of <a href=\"https://stackoverflow.com/questions/5722942/using-virtual-functions-in-cuda-kernels\">this</a> question, which sadly was never fully answered.</p>\n<p>Edit : </p>\n<p>So I found a way to do what I wanted. But just to be clear : This is not at all an answer or solution, the answer was already provided, this is only a hack, just for fun.</p>\n<p>So first lets see what Cuda is doing when calling a virtual method, below is debug SASS</p>\n<pre><code>//R0 is the address of our object\nLD.CG R0, [R0];  \nIADD R0, R0, 0x4;  \nNOP;  \nMOV R0, R0;  \nLD.CG R0, [R0];\n...\nIADD R0, RZ, R9;  \nMOV R0, R0;  \nLDC R0, c[0x2][R0];\n...\nBRX R0 - 0x5478\n</code></pre>\n<p>So assuming that \"c[0x2][INDEX]\" is constant for all kernels we can just get the index for a class by just running a kernel and doing this, where obj is a newly created object of the class looking at:</p>\n<pre><code>unsigned int index = *(unsigned int*)(*(unsigned int*)obj + 4);\n</code></pre>\n<p>Then use something like this :</p>\n<pre><code>struct entry\n{\n    unsigned int vfptr;// := &amp;vfref, thats our value to store in an object\n    int dummy;// := 1234, great for debugging\n    unsigned int vfref;// := &amp;dummy\n    unsigned int index;\n    char ClassName[256];//use it as a key for a dict\n};\n</code></pre>\n<p>Store this in host aswell as device memory(the memory locations are device ones) and on the host you can use the ClassName as a lookup for an object to \"patch\".</p>\n<p>But again : I would not use this in anything serious, because performance wise, virtual functions are not great at all. </p>\n", "PostTypeId": "1", "LastEditorUserId": "-1", "Id": "12701170", "Score": "5", "OwnerUserId": "1715849", "Tags": "<c++><cuda><virtual-functions>", "AnswerCount": "1"}});