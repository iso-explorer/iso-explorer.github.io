post_cb({"27734485": {"ParentId": "27705409", "CommentCount": "0", "Body": "<p>There are quite a few answers here covering implementation, so I'd like to talk about architecture.</p>\n<p><strong>We usually expand 32-bit values to 64-bit values to avoid overflowing because our architectures are designed to handle 64-bit values.</strong></p>\n<p>Most architectures are designed to work with integers whose size is a power of 2 because this makes the hardware vastly simpler.  Tasks such as caching are much simpler this way: there are a large number of divisions and modulus operations which can be replaced with bit masking and shifts if you stick to powers of 2.</p>\n<p>As an example of just how much this matters, The C++11 specification defines multithreading race-cases based on \"memory locations.\"  A memory location is defined in 1.7.3:</p>\n<blockquote>\n<p id=\"so_27705409_27734485_0\">A memory location is either an object of scalar type or a maximal\n  sequence of adjacent bit-fields all having non-zero width.</p>\n</blockquote>\n<p>In other words, if you use C++'s bitfields, you have to do all of your multithreading carefully.  Two adjacent bitfields <em>must</em> be treated as the same memory location, even if you wish computations across them could be spread across multiple threads.  This is very unusual for C++, so likely to cause developer frustration  if you have to worry about it.</p>\n<p>Most processors have a memory architecture which fetches 32-bit or 64-bit blocks of memory at a time.  Thus use of 40-bit values will have a surprising number of extra memory accesses, dramatically affecting run-time.  Consider the alignment issues:</p>\n<pre><code>40-bit word to access:   32-bit accesses   64bit-accesses\nword 0: [0,40)           2                 1\nword 1: [40,80)          2                 2\nword 2: [80,120)         2                 2\nword 3: [120,160)        2                 2\nword 4: [160,200)        2                 2\nword 5: [200,240)        2                 2\nword 6: [240,280)        2                 2\nword 7: [280,320)        2                 1\n</code></pre>\n<p>On a 64 bit architecture, one out of every 4 words will be \"normal speed.\"  The rest will require fetching twice as much data.  If you get a lot of cache misses, this could destroy performance.  Even if you get cache hits, you are going to have to unpack the data and repack it into a 64-bit register to use it (which might even involve a difficult to predict branch).</p>\n<p><strong>It is entirely possible this is worth the cost</strong></p>\n<p>There are situations where these penalties are acceptable.  If you have a large amount of memory-resident data which is well indexed, you may find the memory savings worth the performance penalty.  If you do a large amount of computation on each value, you may find the costs are minimal.  If so, feel free to implement one of the above solutions.  However, here are a few recommendations.</p>\n<ul>\n<li>Do not use bitfields unless you are ready to pay their cost.  For example, if you have an array of bitfields, and wish to divide it up for processing across multiple threads, you're stuck.  By the rules of C++11, the bitfields all form one memory location, so may only be accessed by one thread at a time <em>(this is because the method of packing the bitfields is implementation defined, so C++11 can't help you distribute them in a non-implementation defined manner)</em></li>\n<li>Do not use a structure containing a 32-bit integer and a char to make 40 bytes.  Most processors will enforce alignment and you wont save a single byte.</li>\n<li>Do use homogenous data structures, such as an array of chars or array of 64-bit integers.  It is <em>far</em> easier to get the alignment correct.  <em>(And you also retain control of the packing, which means you can divide an array up amongst several threads for computation if you are careful)</em></li>\n<li>Do design separate solutions for 32-bit and 64-bit processors, if you have to support both platforms.  Because you are doing something very low level and very ill-supported, you'll need to custom tailor each algorithm to its memory architecture.</li>\n<li>Do remember that multiplication of 40-bit numbers is different from multiplication of 64-bit expansions of 40-bit numbers reduced back to 40-bits.  Just like when dealing with the x87 FPU, you have to remember that marshalling your data between bit-sizes changes your result.</li>\n</ul>\n", "OwnerUserId": "2728148", "PostTypeId": "2", "Id": "27734485", "Score": "3", "CreationDate": "2015-01-01T20:43:54.067", "LastActivityDate": "2015-01-01T20:43:54.067"}, "27713173": {"ParentId": "27705409", "PostTypeId": "2", "CommentCount": "4", "Body": "<p>I'll assume that</p>\n<ol>\n<li>this is C, and</li>\n<li>you need a single, large array of 40 bit numbers, and</li>\n<li>you are on a machine that is little-endian, and</li>\n<li>your machine is smart enough to handle alignment</li>\n<li>you have defined size to be the number of 40-bit numbers you need</li>\n</ol>\n<hr>\n<pre><code>unsigned char hugearray[5*size+3];  // +3 avoids overfetch of last element\n\n__int64 get_huge(unsigned index)\n{\n    __int64 t;\n    t = *(__int64 *)(&amp;hugearray[index*5]);\n    if (t &amp; 0x0000008000000000LL)\n        t |= 0xffffff0000000000LL;\n    else\n        t &amp;= 0x000000ffffffffffLL;\n    return t;\n}\n\nvoid set_huge(unsigned index, __int64 value)\n{\n    unsigned char *p = &amp;hugearray[index*5];\n    *(long *)p = value;\n    p[4] = (value &gt;&gt; 32);\n}\n</code></pre>\n<p>It may be faster to handle the get with two shifts.</p>\n<pre><code>__int64 get_huge(unsigned index)\n{\n    return (((*(__int64 *)(&amp;hugearray[index*5])) &lt;&lt; 24) &gt;&gt; 24);\n}\n</code></pre>\n</hr>", "OwnerUserId": "524375", "LastEditorUserId": "995714", "LastEditDate": "2017-03-01T15:10:28.800", "Id": "27713173", "Score": "6", "CreationDate": "2014-12-30T21:58:32.447", "LastActivityDate": "2017-03-01T15:10:28.800"}, "27712474": {"ParentId": "27705409", "PostTypeId": "2", "CommentCount": "1", "Body": "<p>If you have to deal with billions of integers, I'd try to encapuslate <em>arrays</em> of 40-bit numbers instead of <em>single</em> 40-bit numbers. That way, you can test different array implementations (e.g. an implementation that compresses data on the fly, or maybe one that stores less-used data to disk.) without changing the rest of your code. </p>\n<p>Here's a sample implementation (<a href=\"http://rextester.com/SVITH57679\" rel=\"nofollow\">http://rextester.com/SVITH57679</a>): </p>\n<pre><code>class Int64Array\n{\n    char* buffer;\npublic:\n    static const int BYTE_PER_ITEM = 5;\n\n    Int64Array(size_t s)\n    {\n        buffer=(char*)malloc(s*BYTE_PER_ITEM);\n    }\n    ~Int64Array()\n    {\n        free(buffer);\n    }\n\n    class Item\n    {\n        char* dataPtr;\n    public:\n        Item(char* dataPtr) : dataPtr(dataPtr){}\n\n        inline operator int64_t()\n        {\n            int64_t value=0;\n            memcpy(&amp;value, dataPtr, BYTE_PER_ITEM); // Assumes little endian byte order!\n            return value;\n        }\n\n        inline Item&amp; operator = (int64_t value)\n        {\n            memcpy(dataPtr, &amp;value, BYTE_PER_ITEM); // Assumes little endian byte order!\n            return *this;\n        }\n    };   \n\n    inline Item operator[](size_t index) \n    {\n        return Item(buffer+index*BYTE_PER_ITEM);\n    }\n};\n</code></pre>\n<p>Note: The <code>memcpy</code>-conversion from 40-bit to 64-bit is basically undefined behavior, as it assumes litte-endianness. It should work on x86-platforms, though.</p>\n<p>Note 2: Obviously, this is proof-of-concept code, not production-ready code. To use it in real projects, you'd have to add (among other things):</p>\n<ul>\n<li>error handling (malloc can fail!)</li>\n<li>copy constructor (e.g. by copying data, add reference counting or by making the copy constructor private)</li>\n<li>move constructor</li>\n<li>const overloads</li>\n<li>STL-compatible iterators</li>\n<li>bounds checks for indices (in debug build)</li>\n<li>range checks for values (in debug build)</li>\n<li>asserts for the implicit assumptions (little-endianness)</li>\n<li>As it is, <code>Item</code> has reference semantics, not value semantics, which is unusual for <code>operator[]</code>; You could probably work around that with some clever C++ type conversion tricks</li>\n</ul>\n<p>All of those should be straightforward for a C++ programmer, but they would make the sample code much longer without making it clearer, so I've decided to omit them.</p>\n", "OwnerUserId": "143605", "LastEditorUserId": "143605", "LastEditDate": "2015-01-01T13:00:01.590", "Id": "27712474", "Score": "4", "CreationDate": "2014-12-30T20:59:16.423", "LastActivityDate": "2015-01-01T13:00:01.590"}, "27726074": {"ParentId": "27705409", "PostTypeId": "2", "CommentCount": "0", "Body": "<p>This begs for streaming in-memory lossless compression. If this is for a Big Data application, dense packing tricks are tactical solutions at best for what seems to require fairly decent middleware or system-level support. They'd need thorough testing to make sure one is able to recover all the bits unharmed. And the performance implications are highly non-trivial and very hardware-dependent because of interference with the CPU caching architecture (e.g.  cache lines vs packing structure). Someone mentioned complex meshing structures : these are often fine-tuned to cooperate with particular caching architectures.</p>\n<p>It's not clear from the requirements whether the OP needs random access. Given the size of the data it's more likely one would only need local random access on relatively small chunks, organised hierarchically for retrieval. Even the hardware does this at large memory sizes (NUMA). Like lossless movie formats show, it should be possible to get random access in chunks ('frames') without having to load the whole dataset into hot memory (from the compressed in-memory backing store).</p>\n<p>I know of one fast database system (kdb from KX Systems to name one but I know there are others) that can handle extremely large datasets by seemlessly memory-mapping large datasets from backing store. It has the option to transparently compress and expand the data on-the-fly.</p>\n", "OwnerUserId": "4405133", "LastEditorUserId": "4405133", "LastEditDate": "2015-01-02T15:54:49.653", "Id": "27726074", "Score": "2", "CreationDate": "2014-12-31T20:32:07.943", "LastActivityDate": "2015-01-02T15:54:49.653"}, "27713967": {"ParentId": "27705409", "PostTypeId": "2", "CommentCount": "1", "Body": "<p>If what you really want is an array of 40 bit integers (which obviously you can't have), I'd just combine one array of 32 bit and one array of 8 bit integers. </p>\n<p>To read a value x at index i:</p>\n<pre><code>uint64_t x = (((uint64_t) array8 [i]) &lt;&lt; 32) + array32 [i];\n</code></pre>\n<p>To write a value x to index i: </p>\n<pre><code>array8 [i] = x &gt;&gt; 32; array32 [i] = x;\n</code></pre>\n<p>Obviously nicely encapsulated into a class using inline functions for maximum speed. </p>\n<p>There is one situation where this is suboptimal, and that is when you do truly random access to many items, so that each access to an int array would be a cache miss - here you would get two cache misses every time. To avoid this, define a 32 byte struct containing an array of six uint32_t, an array of six uint8_t, and two unused bytes (41 2/3rd bits per number); the code to access an item is slightly more complicated, but both components of the item are in the same cache line. </p>\n", "OwnerUserId": "3255455", "LastEditorUserId": "3255455", "LastEditDate": "2015-01-01T16:38:41.410", "Id": "27713967", "Score": "3", "CreationDate": "2014-12-30T23:17:21.193", "LastActivityDate": "2015-01-01T16:38:41.410"}, "27709220": {"ParentId": "27705409", "PostTypeId": "2", "CommentCount": "2", "Body": "<p>(Edit: First of all - what you want is possible, and makes sense in some cases; I have had to do similar things when I tried to do something for the Netflix challenge and only had 1GB of memory; Second - it is probably best to use a char array for the 40-bit storage to avoid any alignment issues and the need to mess with struct packing pragmas; Third - this design assumes that you're OK with 64-bit arithmetic for intermediate results, it is only for large array storage that you would use Int40; Fourth: I don't get all the suggestions that this is a bad idea, just read up on what people go through to pack mesh data structures and this looks like child's play by comparison).</p>\n<p>What you want is a struct that is only used for storing data as 40-bit ints but implicitly converts to int64_t for arithmetic. The only trick is doing the sign extension from 40 to 64 bits right. If you're fine with unsigned ints, the code can be even simpler. This should be able to get you started.</p>\n<pre><code>#include &lt;cstdint&gt;\n#include &lt;iostream&gt;\n\n// Only intended for storage, automatically promotes to 64-bit for evaluation\nstruct Int40\n{\n     Int40(int64_t x) { set(static_cast&lt;uint64_t&gt;(x)); } // implicit constructor\n     operator int64_t() const { return get(); } // implicit conversion to 64-bit\nprivate:\n     void set(uint64_t x)\n     {\n          setb&lt;0&gt;(x); setb&lt;1&gt;(x); setb&lt;2&gt;(x); setb&lt;3&gt;(x); setb&lt;4&gt;(x);\n     };\n     int64_t get() const\n     {\n          return static_cast&lt;int64_t&gt;(getb&lt;0&gt;() | getb&lt;1&gt;() | getb&lt;2&gt;() | getb&lt;3&gt;() | getb&lt;4&gt;() | signx());\n     };\n     uint64_t signx() const\n     {\n          return (data[4] &gt;&gt; 7) * (uint64_t(((1 &lt;&lt; 25) - 1)) &lt;&lt; 39);\n     };\n     template &lt;int idx&gt; uint64_t getb() const\n     {\n          return static_cast&lt;uint64_t&gt;(data[idx]) &lt;&lt; (8 * idx);\n     }\n     template &lt;int idx&gt; void setb(uint64_t x)\n     {\n          data[idx] = (x &gt;&gt; (8 * idx)) &amp; 0xFF;\n     }\n\n     unsigned char data[5];\n};\n\nint main()\n{\n     Int40 a = -1;\n     Int40 b = -2;\n     Int40 c = 1 &lt;&lt; 16;\n     std::cout &lt;&lt; \"sizeof(Int40) = \" &lt;&lt; sizeof(Int40) &lt;&lt; std::endl;\n     std::cout &lt;&lt; a &lt;&lt; \"+\" &lt;&lt; b &lt;&lt; \"=\" &lt;&lt; (a+b) &lt;&lt; std::endl;\n     std::cout &lt;&lt; c &lt;&lt; \"*\" &lt;&lt; c &lt;&lt; \"=\" &lt;&lt; (c*c) &lt;&lt; std::endl;\n}\n</code></pre>\n<p>Here is the link to try it live: <a href=\"http://rextester.com/QWKQU25252\" rel=\"noreferrer\">http://rextester.com/QWKQU25252</a></p>\n", "OwnerUserId": "4079970", "LastEditorUserId": "4079970", "LastEditDate": "2014-12-30T19:11:23.440", "Id": "27709220", "Score": "19", "CreationDate": "2014-12-30T16:44:01.230", "LastActivityDate": "2014-12-30T19:11:23.440"}, "27705613": {"ParentId": "27705409", "CommentCount": "3", "Body": "<p>Yes, you can do that, and it will save some space for large quantities of numbers</p>\n<p>You need a class that contains a std::vector of an unsigned integer type.</p>\n<p>You will need member functions to store and to retrieve an integer. For example, if you want do store 64 integers of 40 bit each, use a vector of 40 integers of 64 bits each. Then you need a method that stores an integer with index in [0,64] and a method to retrieve such an integer.</p>\n<p>These methods will execute some shift operations, and also some binary | and &amp; .</p>\n<p>I am not adding any more details here yet because your question is not very specific. Do you know how many integers you want to store? Do you know it during compile time? Do you know it when the program starts? How should the integers be organized? Like an array? Like a map? You should know all this before trying to squeeze the integers into less storage.</p>\n", "OwnerUserId": "4362545", "PostTypeId": "2", "Id": "27705613", "Score": "3", "CreationDate": "2014-12-30T12:35:42.850", "LastActivityDate": "2014-12-30T12:35:42.850"}, "27705618": {"ParentId": "27705409", "PostTypeId": "2", "CommentCount": "0", "Body": "<p>As the comments suggest, this is quite a task.  </p>\n<p>Probably an unnecessary hassle <em>unless</em> you want to save alot of RAM - then it makes much more sense. (RAM saving would be the sum total of bits saved across millions of <code>long</code> values stored in RAM)</p>\n<p>I would consider using an array of 5 bytes/char (5 * 8 bits = 40 bits).  Then you will need to shift bits from your (overflowed int - hence a <code>long</code>) value into the array of bytes to store them.</p>\n<p>To use the values, then shift the bits back out into a <code>long</code> and you can use the value.</p>\n<p>Then your RAM and file storage of the value will be 40 bits (5 bytes), BUT you must consider data alignment if you plan to use a <code>struct</code> to hold the 5 bytes.  Let me know if you need elaboration on this bit shifting and data alignment implications.</p>\n<p>Similarly, you could use the 64 bit <code>long</code>, and <em>hide</em> other values (3 chars perhaps) in the residual 24 bits that you do not want to use.  Again - using bit shifting to add and remove the 24 bit values.</p>\n", "OwnerUserId": "1491278", "LastEditorUserId": "1491278", "LastEditDate": "2014-12-30T12:48:29.150", "Id": "27705618", "Score": "8", "CreationDate": "2014-12-30T12:36:25.443", "LastActivityDate": "2014-12-30T12:48:29.150"}, "27705558": {"ParentId": "27705409", "PostTypeId": "2", "CommentCount": "5", "Body": "<p>You can use a bit-field structure, but it's not going to save you any memory:</p>\n<pre><code>struct my_struct\n{\n    unsigned long long a : 40;\n    unsigned long long b : 24;\n};\n</code></pre>\n<p>You can squeeze any multiple of 8 such 40-bit variables into one structure:</p>\n<pre><code>struct bits_16_16_8\n{\n    unsigned short x : 16;\n    unsigned short y : 16;\n    unsigned short z :  8;\n};\n\nstruct bits_8_16_16\n{\n    unsigned short x :  8;\n    unsigned short y : 16;\n    unsigned short z : 16;\n};\n\nstruct my_struct\n{\n    struct bits_16_16_8 a1;\n    struct bits_8_16_16 a2;\n    struct bits_16_16_8 a3;\n    struct bits_8_16_16 a4;\n    struct bits_16_16_8 a5;\n    struct bits_8_16_16 a6;\n    struct bits_16_16_8 a7;\n    struct bits_8_16_16 a8;\n};\n</code></pre>\n<p>This will save you some memory (in comparison with using 8 \"standard\" 64-bit variables), but you will have to split every operation (and in particular arithmetic ones) on each of these variables into several operations.</p>\n<p>So the memory-optimization will be \"traded\" for runtime-performance.</p>\n", "OwnerUserId": "1382251", "LastEditorUserId": "1382251", "LastEditDate": "2014-12-31T20:27:56.303", "Id": "27705558", "Score": "16", "CreationDate": "2014-12-30T12:32:32.167", "LastActivityDate": "2014-12-31T20:27:56.303"}, "27708455": {"ParentId": "27705409", "PostTypeId": "2", "CommentCount": "7", "Body": "<p>You can quite effectively pack 4*40bits integers into a 160-bit struct like this:</p>\n<pre><code>struct Val4 {\n    char hi[4];\n    unsigned int low[4];\n}\n\nlong getLong( const Val4 &amp;pack, int ix ) {\n  int hi= pack.hi[ix];   // preserve sign into 32 bit\n  return long( (((unsigned long)hi) &lt;&lt; 32) + (unsigned long)pack.low[i]);\n}\n\nvoid setLong( Val4 &amp;pack, int ix, long val ) {\n  pack.low[ix]= (unsigned)val;\n  pack.hi[ix]= (char)(val&gt;&gt;32);\n}\n</code></pre>\n<p>These again can be used like this:</p>\n<pre><code>Val4[SIZE] vals;\n\nlong getLong( int ix ) {\n  return getLong( vals[ix&gt;&gt;2], ix&amp;0x3 )\n}\n\nvoid setLong( int ix, long val ) {\n  setLong( vals[ix&gt;&gt;2], ix&amp;0x3, val )\n}\n</code></pre>\n", "OwnerUserId": "448420", "LastEditorUserId": "448420", "LastEditDate": "2017-11-20T16:10:06.493", "Id": "27708455", "Score": "52", "CreationDate": "2014-12-30T15:52:29.023", "LastActivityDate": "2017-11-20T16:10:06.493"}, "27712105": {"ParentId": "27705409", "PostTypeId": "2", "CommentCount": "3", "Body": "<h1>You might want to consider Variable-Lenght Encoding (VLE)</h1>\n<p>Presumably, you have store a lot of those numbers somewhere (in RAM, on disk, send them over the network, etc), and then take them one by one and do some processing.</p>\n<p>One approach would be to <strong>encode</strong> them using VLE.\nFrom Google's protobuf <a href=\"https://developers.google.com/protocol-buffers/docs/encoding\">documentation</a> (CreativeCommons licence)</p>\n<blockquote>\n<p id=\"so_27705409_27712105_0\">Varints are a method of serializing integers using\n  one or more bytes. Smaller numbers take a smaller number of bytes.</p>\n<p id=\"so_27705409_27712105_1\">Each byte in a varint, except the last byte, has the most significant\n  bit (msb) set \u2013 this indicates that there are further bytes to come.\n  The lower 7 bits of each byte are used to store the two's complement\n  representation of the number in groups of 7 bits, least significant\n  group first.</p>\n<p id=\"so_27705409_27712105_2\">So, for example, here is the number 1 \u2013 it's a single byte, so the msb\n  is not set:</p>\n<pre><code>0000 0001\n</code></pre>\n<p id=\"so_27705409_27712105_3\">And here is 300 \u2013 this is a bit more complicated:</p>\n<pre><code>1010 1100 0000 0010\n</code></pre>\n<p id=\"so_27705409_27712105_4\">How do you figure out that this is 300? First you drop the msb from\n  each byte, as this is just there to tell us whether we've reached the\n  end of the number (as you can see, it's set in the first byte as there\n  is more than one byte in the varint)</p>\n</blockquote>\n<p>Pros</p>\n<ul>\n<li>If you have lots of small numbers, you'll probably use less than 40 bytes per integer, in average. Possibly much less.</li>\n<li>You are able to store bigger numbers (with more than 40 bits) in the future, without having to pay a penalty for the small ones</li>\n</ul>\n<p>Cons</p>\n<ul>\n<li>You pay an extra bit for each 7 significant bits of your numbers. That means a number with 40 significant bits will need 6 bytes. If most of your numbers have 40 significant bits, you are better of with a bit field approach.</li>\n<li>You will lose the ability to easily jump to a number given its index (you have to at least partially parse all previous elements in an array in order to access the current one.</li>\n<li>You will need some form of decoding before doing anything useful with the numbers (although that is true for other approaches as well, like bit fields)</li>\n</ul>\n", "OwnerUserId": "981773", "LastEditorUserId": "296974", "LastEditDate": "2014-12-31T08:06:07.933", "Id": "27712105", "Score": "24", "CreationDate": "2014-12-30T20:29:34.990", "LastActivityDate": "2014-12-31T08:06:07.933"}, "27705409": {"CommentCount": "27", "AcceptedAnswerId": "27705562", "PostTypeId": "1", "LastEditorUserId": "1779688", "CreationDate": "2014-12-30T12:22:30.983", "LastActivityDate": "2017-11-20T16:10:06.493", "LastEditDate": "2015-01-02T05:42:51.333", "ViewCount": "8769", "FavoriteCount": "21", "Title": "If a 32-bit integer overflows, can we use a 40-bit structure instead of a 64-bit long one?", "Id": "27705409", "Score": "74", "Body": "<p>If, say, a 32-bit integer is overflowing, instead of upgrading <code>int</code> to <code>long</code>, can we make use of some 40-bit type if we need a range only within 2<sup>40</sup>, so that we save 24 (64-40) bits for every integer?</p>\n<p>If so, how?</p>\n<p>I have to deal with billions and space is a bigger constraint.</p>\n", "Tags": "<c++><c><memory-management><integer-overflow>", "OwnerUserId": "1660982", "AnswerCount": "14"}, "27716570": {"ParentId": "27705409", "CommentCount": "2", "Body": "<p>Another variation that may be helpful would be to use a structure:</p>\n<pre><code>typedef struct TRIPLE_40 {\n  uint32_t low[3];\n  uint8_t hi[3];\n  uint8_t padding;\n};\n</code></pre>\n<p>Such a structure would take 16 bytes and, if 16-byte aligned, would fit entirely within a single cache line.  While identifying which of the parts of the structure to use may be more expensive than it would be if the structure held four elements instead of three, accessing one cache line may be much cheaper than accessing two.  If performance is important, one should use some benchmarks since some machines may perform a divmod-3 operation cheaply and have a high cost per cache-line fetch, while others might have have cheaper memory access and more expensive divmod-3.</p>\n", "OwnerUserId": "363751", "PostTypeId": "2", "Id": "27716570", "Score": "5", "CreationDate": "2014-12-31T05:28:36.877", "LastActivityDate": "2014-12-31T05:28:36.877"}, "27705562": {"ParentId": "27705409", "PostTypeId": "2", "CommentCount": "20", "Body": "<h2>Yes, but...</h2>\n<p>It is certainly <em>possible</em>, but it is usually nonsensical (for any program that doesn't use <em>billions</em> of these numbers):</p>\n<pre><code>#include &lt;stdint.h&gt; // don't want to rely on something like long long\nstruct bad_idea\n{\n    uint64_t var : 40;\n};\n</code></pre>\n<p>Here, <code>var</code> will indeed have a width of 40 bits at the expense of <strike>much</strike> less efficient code generated (it turns out that \"much\" is very much wrong -- the measured overhead is a mere 1-2%, see timings below), and usually to no avail. Unless you have need for another 24-bit value (or an 8 and 16 bit value) which you wish to pack into the same structure, alignment will forfeit anything that you may gain.</p>\n<p>In any case, unless you have billions of these, the effective difference in memory consumption will not be noticeable (but the extra code needed to manage the bit field will be noticeable!).</p>\n<p><strong>Note:</strong><br>\nThe question has in the mean time been updated to reflect that indeed <em>billions</em> of numbers are needed, so this may be a viable thing to do, presumed that you take measures not to lose the gains due to structure alignment and padding, i.e. either by storing something else in the remaining 24 bits or by storing your 40-bit values in structures of 8 each or multiples thereof).<br>\nSaving three bytes <em>a billion times</em> is worthwhile as it will require noticeably fewer memory pages and thus cause fewer cache and TLB misses, and above all page faults (a single page fault weighting tens of millions instructions).</br></br></p>\n<p>While the above snippet does not make use of the remaining 24 bits (it merely demonstrates the \"use 40 bits\" part), something akin to the following will be necessary to really make the approach useful in a sense of preserving memory -- presumed that you indeed have other \"useful\" data to put in the holes:</p>\n<pre><code>struct using_gaps\n{\n    uint64_t var           : 40;\n    uint64_t useful_uint16 : 16;\n    uint64_t char_or_bool  : 8;  \n};\n</code></pre>\n<p>Structure size and alignment will be equal to a 64 bit integer, so nothing is wasted if you make e.g. an array of a billion such structures (even without using compiler-specific extensions). If you don't have use for an 8-bit value, you could also use an 48-bit and a 16-bit value (giving a bigger overflow margin).<br>\nAlternatively you could, at the expense of usability, put 8 40-bit values into a structure (least common multiple of 40 and 64 being 320 = 8*40). Of course then your code which accesses elements in the array of structures will become <strong>much</strong> more complicated (though one could probably implement an <code>operator[]</code> that restores the linear array functionality and hides the structure complexity).</br></p>\n<p><strong>Update:</strong><br>\nWrote a quick test suite, just to see what overhead the bitfields (and operator overloading with bitfield refs) would have. Posted code (due to length) at <a href=\"http://gcc.godbolt.org/#%7B%22version%22%3A3%2C%22filterAsm%22%3A%7B%22labels%22%3Atrue%2C%22directives%22%3Atrue%2C%22commentOnly%22%3Atrue%7D%2C%22compilers%22%3A%5B%7B%22sourcez%22%3A%22MQSwdgxgNgrgJgUwAQB4IAsBOB7M2B8AUKJLIqhAM4AucIBx408yaNcUIARkYTZjAjUk1EAFsEmQgG9CASHYAuRRhx5l6EAHN0AfUwJK2WKNy7o2CAGtloiboAO2cMOoBGANyF5dyQAoASiRFETc%2FJRUsXGwNbT0DIxN6MHMoSxtFPAB3QKDpAF95AD9fTED5WTk5GEpwLSQwAEMJSgdGiGQI1WivKodMFwAzPwAiAFIAWjcANhgRgBokOBhMRtMUiEaaFDEQKE5KBAhcOEp8P00dfUNjGHXU9OVswKQJ0ICAOmOYMGpc3sK%2BS83n4gmEXEacAALAAGXTUGTyGAuaZQ%2BFIABujVgyBCsN6EOhcOE2AckjW2EwAF4%2FMjfqj0eA8pjsTBkFSkOAPEgAPQAKiQlHQ2BgUDgSAwRysSGwGMkgzSWUWXDuSC4UAQ6CQfJ5SAM1BWYG11E0lG5hTkpPJ1EpSDp1AZf2Z%2BsNLJx5sIQO8fGoAiESDa1gQRPhiOqKLRwgxMOCSHxSIj6Ixblj8fD9MjmIATKmYb17Y7MQBmXP5xNRqGlhMZpMAVir6YdmYx0wbBebAHYq17vNAtpQA%2B0rMHroMw4HhyHqAAyAP5sC1LRgYOc34NLwOGDqkAQRTyCcjgzDA9T2cOXTgRY%2FRfL8UuBoXsBBEIOPznpmLMB%2BFJMpDSJCFPIVqrDamB2uW5RyJUChZCA1AYN%2BATyFByFyJshxIDCIQupgRoOB80a9FU6HIG42EIAauEBgRnioSRSBZuRlF4QRWZEWhWzIEWTGuvhGJFux9FQjxVF8VCgmcUgtYiSxGK1hJGHTDJ1EtgpyAdspfEdsCVRyIggyNKK1B7rpeoUa6ea8rqQoimKAY4BC6oAJ5IP2kh%2FDCSFVBagFyCeo4ymSIGUjS7ZJtiXnQZQsHwegiHIdBHEYVhKkxhyWJQNyXAGI0VhqUgZEqSm6XYllOV5XRkmMSpOYlZlarlfl3EqSWdVlQguX5cJKmVm1DUdRVun0dJKn1n12UDflSkqa242NZVGEaSpXZzZNyEWnIOFGnyJogGa8iAsChDUAgYgOFAawICg17aLeq7CAAcvggq%2BmCg5BiGWSrA4QXjkOwawvCAqNJgqxOb0%2FlHoF1qUgA2gAurSC63Su95Pn%2BZnMe9k6jn4INg7DYA8gAHPDn5jMTAQej2x2nedl0oNQTlkk0EhIAAKleyNLqja5PZizjiidNB%2BOzrmg0hlTQaUIhEYMlJIzevPCCASAcpZqsoEgD3cgA1LrIBeShun47DIDw2rnJET53jG42haUDAYiW3myEy9QcsKzdPN3muqvq9ymva3rBtG4ljvO7rHKm%2Bb1vIRixhrHsyBhcIdCUJsmDihyEcArbUWrGAcB%2BG4WZFlCtZU7bcju57ZTe3d97%2B5hgeoMHSD64bCWoaneqW34A%2BpwEhfFwEKAoEWWZBAAenqjRF7kSBjMHqGm5gFsciAcdyL5Bfz8Xpfl5XOn25mEcu0RtfIfL9fc43fsX5ybc6x3ofd7pvdgRyA%2B0uWw%2F74Ecek8Z5zwXgEIIy8daoXPlHcWmBYbr23nIBOF1RAanAjWNOe1M7Z0FE7PO3hAK0zOqgq6Ddlba2egnEA4psRQGFtQSg5RKj9CGKMAASj8MAdQRCGGEDfOBjQXK1AAF7siXjAAAOmABY2sq77gGL8YY4wpjTEoLIkYWR0BrBGFXPyijqDKMmDMdRiwRh0MsLoiGBijGqNMUgEYhwACOfgsgBCsQothKiTEaOcX4TA7i9GsKUaMYxaiNEj1cYE6xXiwn2JGJEgJHj9GxLsRowYBgEDJOCYY0Y0iJgFMKUU4pJTSllPKRUypVTqk1NqWUjxniQkjGkXEjR9pJ7wmSe0rMQM4G6G6b0aW4hJCy2QvjfpLhJ6W2XFkDB1AOnUFhg9eGeca58OupMnp1A%2BSLCenjUGEzfjAMGW7YZYEPbIUQBqE6CM%2BkDIOrbHJyiWlpLManLp5ZgYHPbCcqoV8qjjPbNMhAszU5LJWQ8tZ2xU47MofszAhymx6KGRIc5RErkUQQLcwFEYCEpKaS8nxZjCSA2oMkklxIvkIopb8muZzRkAoORS4FsyKXwnBashhKA2XbN2eccZFLkWnNRQyvSCBrlYotgKyEaZfJPLyWAVpZiTykuyf9T630gooH5l9RoDhaX%2FLkLq%2FC%2BMhEsuxgDYkHLIVcpVXCY1Wqnp8tcd9IVfz6UXKqBim5FtjUfFNeDB5jTcnNLANI6RHjAKgEGPpJAuhdAAHUACSD1J6MFICwVAsEi7YCyJQD46AiDAAQEXEAgwrJxsTSmtNhB7xiEaOAZh8ho2xvjcm1NWYfDDIAEIIC0OAAACpIegB89HFtLeWnkuo23Vs7fIOhDDKBayQCuldbgYRZihHyddm7zh6IXXwpdBVWwCh3Vus9e7egHpoEu0uxNtQFQ3eep9l6K14GECWkUOgkDsIAIIAFkZRGjEC5etGBwDIAEVwbQWgRmHARM2straq0dq7RIAAokXIdAxsCjt6OOugk7p0oZrauzGFkvCFCAAAA%3D%3D%22%2C%22compiler%22%3A%22%2Fusr%2Fbin%2Fg%2B%2B-4.8%22%2C%22options%22%3A%22-O%20-std%3Dc%2B%2B0x%22%7D%5D%7D\">gcc.godbolt.org</a>, test output from my Win7-64 machine is:</br></p>\n<pre><code>Running test for array size = 1048576\nwhat       alloc   seq(w)  seq(r)  rand(w)  rand(r)  free\n-----------------------------------------------------------\nuint32_t    0      2       1       35       35       1\nuint64_t    0      3       3       35       35       1\nbad40_t     0      5       3       35       35       1\npacked40_t  0      7       4       48       49       1\n\n\nRunning test for array size = 16777216\nwhat        alloc  seq(w)  seq(r)  rand(w)  rand(r)  free\n-----------------------------------------------------------\nuint32_t    0      38      14      560      555      8\nuint64_t    0      81      22      565      554      17\nbad40_t     0      85      25      565      561      16\npacked40_t  0      151     75      765      774      16\n\n\nRunning test for array size = 134217728\nwhat        alloc  seq(w)  seq(r)  rand(w)  rand(r)  free\n-----------------------------------------------------------\nuint32_t    0      312     100     4480     4441     65\nuint64_t    0      648     172     4482     4490     130\nbad40_t     0      682     193     4573     4492     130\npacked40_t  0      1164    552     6181     6176     130\n</code></pre>\n<p>What one can see is that the extra overhead of bitfields is neglegible, but the operator overloading with bitfield reference as a convenience thing is rather drastic (about 3x increase) when accessing data linearly in a cache-friendly manner. On the other hand, on random access it barely even matters.</p>\n<p>These timings suggest that simply using 64-bit integers would be better since they are still faster overall than bitfields (despite touching more memory), but of course they do not take into account the cost of page faults with much bigger datasets. It might look very different once you run out of physical RAM (I didn't test that).</p>\n", "OwnerUserId": "572743", "LastEditorUserId": "5801", "LastEditDate": "2015-01-08T13:43:38.383", "Id": "27705562", "Score": "82", "CreationDate": "2014-12-30T12:32:40.713", "LastActivityDate": "2015-01-08T13:43:38.383"}, "bq_ids": {"n4140": {"so_27705409_27734485_0": {"section_id": 5787, "quality": 0.9285714285714286, "length": 13}}, "n3337": {"so_27705409_27734485_0": {"section_id": 5560, "quality": 0.9285714285714286, "length": 13}}, "n4659": {"so_27705409_27734485_0": {"section_id": 7244, "quality": 0.9285714285714286, "length": 13}}}, "27727570": {"ParentId": "27705409", "PostTypeId": "2", "CommentCount": "0", "Body": "<p>For the case of storing some billions of 40-bit signed integers, and assuming 8-bit bytes, you can pack 8 40-bit signed integers in a struct (in the code below using an array of bytes to do that), and, since this struct is ordinarily aligned, you can then create a logical array of such packed groups, and provide ordinary sequential indexing of that:</p>\n<pre><code>#include &lt;limits.h&gt;     // CHAR_BIT\n#include &lt;stdint.h&gt;     // int64_t\n#include &lt;stdlib.h&gt;     // div, div_t, ptrdiff_t\n#include &lt;vector&gt;       // std::vector\n\n#define STATIC_ASSERT( e ) static_assert( e, #e )\n\nnamespace cppx {\n    using Byte = unsigned char;\n    using Index = ptrdiff_t;\n    using Size = Index;\n\n    // For non-negative values:\n    auto roundup_div( const int64_t a, const int64_t b )\n        -&gt; int64_t\n    { return (a + b - 1)/b; }\n\n}  // namespace cppx\n\nnamespace int40 {\n    using cppx::Byte;\n    using cppx::Index;\n    using cppx::Size;\n    using cppx::roundup_div;\n    using std::vector;\n\n    STATIC_ASSERT( CHAR_BIT == 8 );\n    STATIC_ASSERT( sizeof( int64_t ) == 8 );\n\n    const int bits_per_value    = 40;\n    const int bytes_per_value   = bits_per_value/8;\n\n    struct Packed_values\n    {\n        enum{ n = sizeof( int64_t ) };\n        Byte bytes[n*bytes_per_value];\n\n        auto value( const int i ) const\n            -&gt; int64_t\n        {\n            int64_t result = 0;\n            for( int j = bytes_per_value - 1; j &gt;= 0; --j )\n            {\n                result = (result &lt;&lt; 8) | bytes[i*bytes_per_value + j];\n            }\n            const int64_t first_negative = int64_t( 1 ) &lt;&lt; (bits_per_value - 1);\n            if( result &gt;= first_negative )\n            {\n                result = (int64_t( -1 ) &lt;&lt; bits_per_value) | result;\n            }\n            return result;\n        }\n\n        void set_value( const int i, int64_t value )\n        {\n            for( int j = 0; j &lt; bytes_per_value; ++j )\n            {\n                bytes[i*bytes_per_value + j] = value &amp; 0xFF;\n                value &gt;&gt;= 8;\n            }\n        }\n    };\n\n    STATIC_ASSERT( sizeof( Packed_values ) == bytes_per_value*Packed_values::n );\n\n    class Packed_vector\n    {\n    private:\n        Size                    size_;\n        vector&lt;Packed_values&gt;   data_;\n\n    public:\n        auto size() const -&gt; Size { return size_; }\n\n        auto value( const Index i ) const\n            -&gt; int64_t\n        {\n            const auto where = div( i, Packed_values::n );\n            return data_[where.quot].value( where.rem );\n        }\n\n        void set_value( const Index i, const int64_t value ) \n        {\n            const auto where = div( i, Packed_values::n );\n            data_[where.quot].set_value( where.rem, value );\n        }\n\n        Packed_vector( const Size size )\n            : size_( size )\n            , data_( roundup_div( size, Packed_values::n ) )\n        {}\n    };\n\n}    // namespace int40\n\n#include &lt;iostream&gt;\nauto main() -&gt; int\n{\n    using namespace std;\n\n    cout &lt;&lt; \"Size of struct is \" &lt;&lt; sizeof( int40::Packed_values ) &lt;&lt; endl;\n    int40::Packed_vector values( 25 );\n    for( int i = 0; i &lt; values.size(); ++i )\n    {\n        values.set_value( i, i - 10 );\n    }\n    for( int i = 0; i &lt; values.size(); ++i )\n    {\n        cout &lt;&lt; values.value( i ) &lt;&lt; \" \";\n    }\n    cout &lt;&lt; endl;\n}\n</code></pre>\n", "OwnerUserId": "464581", "LastEditorUserId": "464581", "LastEditDate": "2015-01-01T01:04:05.417", "Id": "27727570", "Score": "5", "CreationDate": "2015-01-01T00:47:47.793", "LastActivityDate": "2015-01-01T01:04:05.417"}});