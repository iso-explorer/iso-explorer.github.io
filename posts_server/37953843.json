post_cb({"37953843": {"CommentCount": "10", "AcceptedAnswerId": "37955020", "PostTypeId": "1", "LastEditorUserId": "-1", "CreationDate": "2016-06-21T20:18:53.643", "LastActivityDate": "2016-06-21T23:50:45.720", "LastEditDate": "2017-05-23T11:44:16.277", "ViewCount": "106", "FavoriteCount": "1", "Title": "How can force the user/OS to input an Ascii string", "Id": "37953843", "Score": "2", "Body": "<p>This is an extended question of this one: <a href=\"https://stackoverflow.com/questions/37953246/is-stdstring-suppose-to-have-only-ascii-characters\">Is std::string suppose to have only Ascii characters</a></p>\n<p>I want to build a simple console application that take an input from the user as set of characters. Those characters include <code>0-&gt;9</code> digits and <code>a-&gt;z</code> letters.</p>\n<p>I am dealing with input supposing that it is an Ascii. For example, I am using something like : <code>static_cast&lt;unsigned int&gt;(my_char - '0')</code> to get the number as <code>unsigned int</code>.</p>\n<p>How can I make this code cross-platform? How can tell that I want the input to be Ascii always? Or I have missed a lot of concepts and <code>static_cast&lt;unsigned int&gt;(my_char - '0')</code> is just a bad way?</p>\n<p>P.S. In Ascii (at least) digits have sequenced order. However, in others encoding, I do not know if they have. (I am pretty sure that they are but there is no guarantee, right?) </p>\n", "Tags": "<c++><string><c++11><ascii>", "OwnerUserId": "4523099", "AnswerCount": "2"}, "37955020": {"CommentCount": "4", "CreationDate": "2016-06-21T21:39:07.483", "CommunityOwnedDate": "2016-06-21T23:53:45.587", "LastEditorUserId": "4342498", "LastActivityDate": "2016-06-21T23:50:45.720", "ParentId": "37953843", "PostTypeId": "2", "LastEditDate": "2016-06-21T23:50:45.720", "Id": "37955020", "Score": "2", "Body": "<blockquote>\n<p id=\"so_37953843_37955020_0\">How can force the user/OS to input an Ascii string</p>\n</blockquote>\n<p>You cannot, unless you let the user specify the numeric values of such ASCII input.</p>\n<p>It all depends how the terminal implementation used to serve <code>std::cin</code> translates key strokes like <code>0</code> to a specific number, and what your toolchain expects to match that number with it's intrinsic translation for <code>'0'</code>.</p>\n<p>You simply shouldn't expect ASCII values explicitly (e.g. using magic numbers), but <code>char</code> literals to provide portable code. The assumption that <code>my_char - '0'</code> will result in the actual digits value is true for all character sets.  The C++ standard states in [lex.charset]/3 that</p>\n<blockquote>\n<p id=\"so_37953843_37955020_1\">The basic execution character set and the basic execution wide-character set shall each contain all the members of the basic source character set, plus control characters representing alert, backspace, and carriage return, plus a null character (respectively, null wide character), whose representation has all zero bits. For each basic execution character set, the values of the members shall be non-negative and distinct from one another. <strong>In both the source and execution basic character sets, the value of each character after 0 in the above list of decimal digits shall be one greater than the value of the previous.</strong>[...]</p>\n</blockquote>\n<p><sup>emphasis mine</sup></p>\n", "OwnerUserId": "1413395"}, "37955583": {"ParentId": "37953843", "CommentCount": "0", "Body": "<p>You can't force or even verify that beforehand . \"Evil user\" can always sneak a UTF-8 encoded string into your application, with no characters above U+7F. And such string happens to be also Ascii-encoded.</p>\n<p>Also, whatever platform specific measure you take, user can pipe a UTF-16LE encoded file. Or <code>/dev/urandom</code></p>\n<p>Your mistakes string encoding with some magic property of an input stream - and it is not. It is, well, encoding, like JPEG or AVI, and must be handled exactly the same way - read an input, match with format, report errors on parsing failure. </p>\n<p>For your case, if you want to accept only ASCII, read input stream byte by byte and throw/exit with error if you ever encounters a byte with the value outside ASCII domain.</p>\n<p>However, if later you encounter a terminal providing data with some incompatible encoding, like UTF16LE, you have no choice but to write a detection (based on byte order mark) and a conversion routine.</p>\n", "OwnerUserId": "2512323", "PostTypeId": "2", "Id": "37955583", "Score": "1", "CreationDate": "2016-06-21T22:24:10.977", "LastActivityDate": "2016-06-21T22:24:10.977"}, "bq_ids": {"n4140": {"so_37953843_37955020_1": {"section_id": 5316, "quality": 0.9117647058823529, "length": 62}}, "n3337": {"so_37953843_37955020_1": {"section_id": 5113, "quality": 0.9117647058823529, "length": 62}}, "n4659": {"so_37953843_37955020_1": {"section_id": 6740, "quality": 0.7941176470588235, "length": 54}}}});